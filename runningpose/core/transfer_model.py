# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/20_transfer_model.ipynb (unless otherwise specified).

__all__ = ['keypoints_3D', 'keypoints_3D', 'joints_left', 'joints_right', 'keypoints_2D', 'keypoints_2D_metadata',
           'keypoints_2D_symmetry', 'kps_left', 'kps_right', 'keypoints_2D', 'keypoints_2D', 'subjects',
           'subjects_extra_cut', 'subjects_cut', 'keypoints_3D_val', 'keypoints_3D_val', 'keypoints_2D_val',
           'keypoints_2D_val', 'keypoints_2D_val', 'subjects', 'subjects_cut', 'shapes_3d', 'shapes_2d',
           'poses_2d_train', 'poses_3d_train', 'poses_2d_val', 'poses_3d_val', 'checkpoint', 'num_joints_in',
           'in_features', 'num_joints_out', 'filter_widths', 'causal', 'dropout', 'channels', 'lr', 'lr_decay',
           'batch_size', 'chunk_length', 'num_epochs', 'trigger_times', 'patience', 'model_run_train', 'model_run',
           'receptive_field', 'pad', 'optimizer', 'scaler', 'losses_3d_train', 'losses_3d_train_eval',
           'losses_3d_valid', 'initial_momentum', 'final_momentum', 'valid_generator', 'train_generator',
           'train_generator_eval', 'epoch', 'chk_path']

# Cell
import os
import time

import matplotlib
import numpy as np
import torch

matplotlib.use('Agg')
import matplotlib.pyplot as plt
import torch.optim as optim
import torch.nn as nn

from .generators import ChunkedGenerator, UnchunkedGenerator
from .loss import mpjpe
from .model import TemporalModel
from .camera import normalize_screen_coordinates

# Cell
print('Loading training dataset...')
keypoints_3D = np.load('data_3d_train.npz', allow_pickle=True)
keypoints_3D = keypoints_3D['positions_3d'].item()
joints_left = [1, 3, 5, 7, 10, 11, 14]
joints_right = [0, 2, 4, 6, 8, 9, 13]
print(keypoints_3D.keys())
print(keypoints_3D['Ioanna3_Camera1_170Hz_3D_keypoints'].shape)

# Cell
print('Loading 2D training detections...')
keypoints_2D = np.load('data_2d_custom_trainingdata.npz', allow_pickle=True)
keypoints_2D_metadata = keypoints_2D['metadata'].item()
keypoints_2D_symmetry = keypoints_2D_metadata['keypoints_symmetry']
kps_left = list(keypoints_2D_symmetry[0])
kps_right = list(keypoints_2D_symmetry[1])
keypoints_2D = keypoints_2D['positions_2d'].item()
keypoints_2D = dict(sorted(keypoints_2D.items()))
print(keypoints_2D.keys())
print(keypoints_2D['miqus1_Ioanna_01.avi']['custom'][0].shape)

# Cell
# Normalize camera frame
subjects = [
    'miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi',
    'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi',
    'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi',
    'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi',
    'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi'
]
# 2D data
for subject in subjects:
    for action in keypoints_2D[subject]:
        for idx, kps in enumerate(keypoints_2D[subject][action]):
            kps = normalize_screen_coordinates(kps, w=1920, h=1088)
            keypoints_2D[subject][action][idx] = kps

# Cell
subjects_extra_cut = ['miqus3_Ioanna_01.avi', 'miqus3_Josef_01.avi']
for subject in subjects_extra_cut:
    for action in keypoints_2D[subject]:
        for idx, kps in enumerate(keypoints_2D[subject][action]):
            kps = normalize_screen_coordinates(kps, w=1350, h=1088)
            keypoints_2D[subject][action][idx] = kps

# Cell
subjects_cut = [
    'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_02.avi',
    'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'
]
for subject in subjects_cut:
    for action in keypoints_2D[subject]:
        for idx, kps in enumerate(keypoints_2D[subject][action]):
                kps = normalize_screen_coordinates(kps, w=1480, h=1088)
                keypoints_2D[subject][action][idx] = kps

# Cell
print('Loading validation dataset...')
keypoints_3D_val = np.load('data_3d_val.npz', allow_pickle=True)
keypoints_3D_val = keypoints_3D_val['positions_3d'].item()
print(keypoints_3D_val.keys())
# print(keypoints_3D['Ioanna1_Camera1_170Hz_3D_keypoints'].shape)

# Cell
print('Loading 2D training detections...')
keypoints_2D_val = np.load('data_2d_custom_validationdata.npz', allow_pickle=True)
keypoints_2D_val = keypoints_2D_val['positions_2d'].item()
keypoints_2D_val = dict(sorted(keypoints_2D_val.items()))
print(keypoints_2D_val.keys())

# Cell
subjects = [
    'miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi',
    'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi'
]
for subject in subjects:
    for action in keypoints_2D_val[subject]:
        for idx, kps in enumerate(keypoints_2D_val[subject][action]):
            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1920, h=1088)
            keypoints_2D_val[subject][action][idx] = kps

# Cell
subjects_cut = [
    'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi',
    'miqus3_Tindra_03.avi'
]
for subject in subjects_cut:
    for action in keypoints_2D_val[subject]:
        for idx, kps in enumerate(keypoints_2D_val[subject][action]):
            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1480, h=1088)
            keypoints_2D_val[subject][action][idx] = kps

# Cell
# Wrong cut .avi should have been 4.42 long but is 4.44
keypoints_2D['miqus3_Josef_03.avi']['custom'][0] = keypoints_2D['miqus3_Josef_03.avi']['custom'][0][:374]

# Cell
shapes_3d = []
for subject in keypoints_3D.keys():
    shapes_3d.append(keypoints_3D[subject].shape)

# Cell
keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][20, 9]
keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][20, 9]
keypoints_3D['Ioanna2_Camera3_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna3_Camera2_170Hz_3D_keypoints'][20, 9]

# Cell
shapes_2d = []
for subject in keypoints_2D.keys():
    shapes_2d.append(keypoints_2D[subject]['custom'][0].shape)

# Cell
for i in range(len(shapes_2d)):
    assert shapes_2d[i][0] == shapes_3d[i][0], f'subject {i}: {shapes_2d[i][0]}, {shapes_3d[i][0]}'

# Cell
poses_2d_train = []
for subject in keypoints_2D.keys():
    poses_2d_train.append(keypoints_2D[subject]['custom'][0])

poses_3d_train = []
for subject in keypoints_3D.keys():
    poses_3d_train.append(keypoints_3D[subject])

assert len(poses_2d_train) == len(poses_3d_train), "Number of runs doesn't match."

poses_2d_val = []
for subject in keypoints_2D_val.keys():
    poses_2d_val.append(keypoints_2D_val[subject]['custom'][0])

poses_3d_val = []
for subject in keypoints_3D_val.keys():
    poses_3d_val.append(keypoints_3D_val[subject])

assert len(poses_2d_val) == len(poses_3d_val), "Number of runs doesn't match."

# Cell
# Load checkpoint
print('Loading checkpoint')
checkpoint = torch.load('pretrained_h36m_detectron_coco.bin',
                        map_location=lambda storage,
                        loc: storage)
print('This model was trained for {} epochs'.format(checkpoint['epoch']))

# Cell
# Hyperparameters
num_joints_in = 17 # COCO
in_features = 2 # dimension of in joints
num_joints_out = 18
filter_widths = [3, 3, 3, 3, 3] # just as in inference
causal = False # No real time predictions
dropout = 0.333
channels = 1024 # default
lr = 1e-3
lr_decay = 0.99
batch_size = 256
chunk_length = 1
num_epochs = 2500
trigger_times = 0
patience = 4

# Load two models one for training and one for evaluation
model_run_train = TemporalModel(
    num_joints_in, in_features, num_joints_out, filter_widths, causal,
    dropout, channels
)
model_run = TemporalModel(
    num_joints_in, in_features, num_joints_out, filter_widths, causal,
    dropout, channels
)

if torch.cuda.is_available():
    model_run_train = model_run_train.cuda()
    model_run = model_run.cuda()

# Reintizialize the last output layer to fit new out.
checkpoint['model_pos']['shrink.weight'] = torch.randn(num_joints_out*3, channels, 1)
checkpoint['model_pos']['shrink.bias'] = torch.from_numpy(mean_keypoints)

# Load the pretrained model i.e to do transfer learning
model_run_train.load_state_dict(checkpoint['model_pos'])

# Cell
# Calculate padding based on receptive field
receptive_field = model_run_train.receptive_field()
print('INFO: Receptive field: {} frames'.format(receptive_field))
pad = (receptive_field - 1) // 2 # Padding on each side

# Cell
# Optimizer
optimizer = optim.Adam(model_run_train.parameters(), lr=lr, amsgrad=True)
scaler = torch.cuda.amp.GradScaler()
# Initialize loss
losses_3d_train = []
losses_3d_train_eval = []
losses_3d_valid = []

# Using batch norm momentum
initial_momentum = 0.1
final_momentum = 0.001

# Cell
valid_generator = UnchunkedGenerator(
    cameras=None, poses_3d=poses_3d_val, poses_2d=poses_2d_val,
    pad=pad, augment=False,
    kps_left=kps_left, kps_right=kps_right,
    joints_left=joints_left, joints_right=joints_right
)
print('INFO: Testing on {} frames'.format(valid_generator.num_frames()))

train_generator = ChunkedGenerator(
    batch_size, cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train,
    pad=pad, chunk_length=chunk_length, shuffle=True, augment=True,
    kps_left=kps_left, kps_right=kps_right,
    joints_left=joints_left, joints_right=joints_right
)
train_generator_eval = UnchunkedGenerator(
    cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train,
    pad=pad, augment=False
)
print('INFO: Training on {} frames'.format(train_generator_eval.num_frames()))

# Cell
epoch = 0
while epoch < num_epochs:
    start_time = time.time()
    # Initialize training loss
    epoch_loss_3d_train = 0
    epoch_loss_2d_train_unlabeled = 0
    N = 0
    # Regular supervised scenario
    for _, batch_3d, batch_2d in train_generator.next_epoch():
        inputs_3d = torch.from_numpy(batch_3d.astype('float32'))
        inputs_2d = torch.from_numpy(batch_2d.astype('float32'))
        if torch.cuda.is_available():
            inputs_3d = inputs_3d.cuda()
            inputs_2d = inputs_2d.cuda()

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Predict 3D poses (forward) using fp16
        with torch.cuda.amp.autocast():
            predicted_3d_pos = model_run_train(inputs_2d)
            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
            N += inputs_3d.shape[0]
            epoch_loss_3d_train += inputs_3d.shape[0] * loss_3d_pos.item()

        # Backward
        scaler.scale(loss_3d_pos).backward()
        scaler.step(optimizer)
        scaler.update()


    # Total loss over one epoch
    losses_3d_train.append(epoch_loss_3d_train / N)

    # End-of-epoch evaluation
    with torch.no_grad():
        # Load the newly trained network
        model_run.load_state_dict(model_run_train.state_dict())
        model_run.eval()
        # Initialize validation loss
        epoch_loss_3d_valid = 0
        epoch_loss_2d_valid = 0
        N = 0

        # Evaluate on validation dataset
        for _, batch_3d, batch_2d in valid_generator.next_epoch():
            inputs_3d_valid = torch.from_numpy(batch_3d.astype('float32'))
            inputs_2d_valid = torch.from_numpy(batch_2d.astype('float32'))
            if torch.cuda.is_available():
                inputs_3d_valid = inputs_3d_valid.cuda()
                inputs_2d_valid = inputs_2d_valid.cuda()

            # Predict 3D poses (forward)
            predicted_3d_pos = model_run(inputs_2d_valid)
            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d_valid)
            N += inputs_3d_valid.shape[0]
            epoch_loss_3d_valid += inputs_3d_valid.shape[0] * loss_3d_pos.item()

        # Total loss over one epoch
        losses_3d_valid.append(epoch_loss_3d_valid / N)

        # # Early stopping
        # if epoch > 1:
        #     if losses_3d_valid[-1] > losses_3d_valid[-2]:
        #         trigger_times += 1
        #         print('Trigger Times:', trigger_times, flush=True)

        #         if trigger_times > patience:
        #             print('Early stopping! at epoch:', epoch+1)
        #             epoch = num_epochs


        # Evaluate on training set, this time in evaluation mode
        epoch_loss_3d_train_eval = 0
        epoch_loss_2d_train_labeled_eval = 0
        N = 0
        for _, batch_3d, batch_2d in train_generator_eval.next_epoch():
            if batch_2d.shape[1] == 0:
                # This can only happen when downsampling the dataset
                continue

            inputs_3d = torch.from_numpy(batch_3d.astype('float32'))
            inputs_2d = torch.from_numpy(batch_2d.astype('float32'))
            if torch.cuda.is_available():
                inputs_3d = inputs_3d.cuda()
                inputs_2d = inputs_2d.cuda()

            # Predict 3D poses (forward)
            predicted_3d_pos = model_run(inputs_2d)
            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)
            N += inputs_3d.shape[0]
            epoch_loss_3d_train_eval += inputs_3d.shape[0] * loss_3d_pos.item()

        # Total loss over one epoch
        losses_3d_train_eval.append(epoch_loss_3d_train_eval / N)

    # Calculate total training/validation time over one epoch
    elapsed = time.time() - start_time

    print(
        f'''[{epoch+1}] time {elapsed:.2f} lr {lr}
        3d_train {losses_3d_train[-1] * 1000}
        3d_eval {losses_3d_train_eval[-1] * 1000}
        3d_valid {losses_3d_valid[-1]  *1000}''',
        flush=True
    )

    # Decay learning rate exponentially
    lr *= lr_decay
    for param_group in optimizer.param_groups:
        param_group['lr'] *= lr_decay
    epoch += 1

    # Decay BatchNorm momentum
    momentum = initial_momentum * np.exp(
        -epoch/num_epochs * np.log(initial_momentum/final_momentum)
    )
    model_run_train.set_bn_momentum(momentum)

    # Save training curves after every epoch, as .png images
    if epoch >= num_epochs:
        plt.figure()
        epoch_x = np.arange(3, len(losses_3d_train)) + 1
        plt.plot(epoch_x, losses_3d_train[3:], '--', color='C0')
        plt.plot(epoch_x, losses_3d_train_eval[3:], color='C0')
        plt.plot(epoch_x, losses_3d_valid[3:], color='C1')
        plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])
        plt.ylabel('MPJPE (mm)')
        plt.xlabel('Epoch')
        plt.xlim((3, epoch))
        plt.savefig('loss_plots/' + str(epoch) + '_loss_3d.png')
        plt.close('all')

# Cell
chk_path = os.path.join('runningpose_epoch_{}.bin'.format(epoch))
print(chk_path)
print('Saving checkpoint to', chk_path)
torch.save({
    'epoch': epoch,
    'lr': lr,
    'random_state': train_generator.random_state(),
    'optimizer': optimizer.state_dict(),
    'model_run': model_run_train.state_dict(),
}, chk_path)