{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera\n",
    "> A camera file with a range of functions to handle the camera and it's coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from runningpose.core.quaternion import qinverse, qrot\n",
    "from runningpose.core.utils import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def normalize_screen_coordinates(X, w, h): \n",
    "    \"\"\"\n",
    "    Normalize so that [0, w] is mapped to [-1, 1], while preserving the \n",
    "    aspect ratio.\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == 2\n",
    "    \n",
    "    return 2*(X/w) - [1, h/w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def image_coordinates(X, w, h):\n",
    "    \"\"\"Return the image coordinates\"\"\"\n",
    "    assert X.shape[-1] == 2\n",
    "    # Reverse camera frame normalization\n",
    "    return (X + [1, h/w])*w/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def world_to_camera(X, R, t):\n",
    "    \"\"\"\n",
    "    Converts from world to camera coordinates with Human3.6M extrinsic \n",
    "    camera parameters structure.\n",
    "    \"\"\"\n",
    "    Rt = wrap(qinverse, R) # Invert rotation\n",
    "    # Rotate and translate\n",
    "    return wrap(qrot, np.tile(Rt, (*X.shape[:-1], 1)), X - t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def camera_to_world(X, R, t):\n",
    "    \"\"\"\n",
    "    Converts from camera to world coordinates with Human3.6M extrinsic \n",
    "    camera parameters structure.\n",
    "    \"\"\"\n",
    "    # Rotate and translate\n",
    "    return wrap(qrot, np.tile(R, (*X.shape[:-1], 1)), X) + t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def project_to_2d(X, camera_params):\n",
    "    \"\"\"\n",
    "    Project 3D points to 2D using the Human3.6M camera projection \n",
    "    function.\n",
    "    This is a differentiable and batched reimplementation of the \n",
    "    original MATLAB script.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- 3D points in *camera space* to transform (N, *, 3)\n",
    "    camera_params -- intrinsic parameteres (N, 2+2+3+2=9)\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == 3\n",
    "    assert len(camera_params.shape) == 2\n",
    "    assert camera_params.shape[-1] == 9\n",
    "    assert X.shape[0] == camera_params.shape[0]\n",
    "    \n",
    "    while len(camera_params.shape) < len(X.shape):\n",
    "        camera_params = camera_params.unsqueeze(1)\n",
    "        \n",
    "    f = camera_params[..., :2]\n",
    "    c = camera_params[..., 2:4]\n",
    "    k = camera_params[..., 4:7]\n",
    "    p = camera_params[..., 7:]\n",
    "    \n",
    "    XX = torch.clamp(X[..., :2] / X[..., 2:], min=-1, max=1)\n",
    "    r2 = torch.sum(XX[..., :2]**2, dim=len(XX.shape)-1, keepdim=True)\n",
    "\n",
    "    radial = 1 + torch.sum(k * torch.cat((r2, r2**2, r2**3), \n",
    "                           dim=len(r2.shape)-1), \n",
    "                           dim=len(r2.shape)-1, \n",
    "                           keepdim=True)\n",
    "    tan = torch.sum(p*XX, dim=len(XX.shape)-1, keepdim=True)\n",
    "\n",
    "    XXX = XX*(radial + tan) + p*r2\n",
    "    \n",
    "    return f*XXX + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def project_to_2d_linear(X, camera_params):\n",
    "    \"\"\"\n",
    "    Project 3D points to 2D using only linear parameters \n",
    "    (focal length and principal point).\n",
    "    \n",
    "    Arguments:\n",
    "    X -- 3D points in *camera space* to transform (N, *, 3)\n",
    "    camera_params -- intrinsic parameteres (N, 2+2+3+2=9)\n",
    "    \"\"\"\n",
    "    assert X.shape[-1] == 3\n",
    "    assert len(camera_params.shape) == 2\n",
    "    assert camera_params.shape[-1] == 9\n",
    "    assert X.shape[0] == camera_params.shape[0]\n",
    "    \n",
    "    while len(camera_params.shape) < len(X.shape):\n",
    "        camera_params = camera_params.unsqueeze(1)\n",
    "        \n",
    "    f = camera_params[..., :2]\n",
    "    c = camera_params[..., 2:4]\n",
    "    \n",
    "    XX = torch.clamp(X[..., :2] / X[..., 2:], min=-1, max=1)\n",
    "    \n",
    "    return f*XX + c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miqus Camera\n",
    "> Functions that handles extrinsic camera parameters from Qualisys calibration file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def world_to_camera_miqus(P, R, T):\n",
    "  \"\"\"\n",
    "  Convert points from world to camera coordinates\n",
    "  Args\n",
    "    P: Nx3 3d points in world coordinates\n",
    "    R: 3x3 Camera rotation matrix\n",
    "    T: 3x1 Camera translation parameters\n",
    "  Returns\n",
    "    X_cam: Nx3 3d points in camera coordinates\n",
    "  \"\"\"\n",
    "\n",
    "  assert len(P.shape) == 2\n",
    "  assert P.shape[1] == 3\n",
    "\n",
    "  X_cam = R.dot( P.T - T ) # rotate and translate\n",
    "\n",
    "  return X_cam.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "def camera_to_world_miqus(P, R, T):\n",
    "  \"\"\"Inverse of world_to_camera_frame\n",
    "  Args\n",
    "    P: Nx3 points in camera coordinates\n",
    "    R: 3x3 Camera rotation matrix\n",
    "    T: 3x1 Camera translation parameters\n",
    "  Returns\n",
    "    X_cam: Nx3 points in world coordinates\n",
    "  \"\"\"\n",
    "\n",
    "  assert len(P.shape) == 2\n",
    "  assert P.shape[1] == 3\n",
    "\n",
    "  X_cam = R.T.dot( P.T ) + T # rotate and translate\n",
    "\n",
    "  return X_cam.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def project_point_radial(P, R, T, f, c, k, p):\n",
    "  \"\"\"\n",
    "  Project points from 3d to 2d using camera parameters\n",
    "  including radial and tangential distortion\n",
    "  Args\n",
    "    P: Nx3 points in world coordinates\n",
    "    R: 3x3 Camera rotation matrix\n",
    "    T: 3x1 Camera translation parameters\n",
    "    f: (scalar) Camera focal length\n",
    "    c: 2x1 Camera center\n",
    "    k: 3x1 Camera radial distortion coefficients\n",
    "    p: 2x1 Camera tangential distortion coefficients\n",
    "  Returns\n",
    "    Proj: Nx2 points in pixel space\n",
    "    D: 1xN depth of each point in camera space\n",
    "    radial: 1xN radial distortion per point\n",
    "    tan: 1xN tangential distortion per point\n",
    "    r2: 1xN squared radius of the projected points before distortion\n",
    "  \"\"\"\n",
    "\n",
    "  # P is a matrix of 3-dimensional points\n",
    "  assert len(P.shape) == 2\n",
    "  assert P.shape[1] == 3\n",
    "\n",
    "  N = P.shape[0]\n",
    "  X = R.dot(P.T - T) # rotate and translate\n",
    "  XX = X[:2,:] / X[2,:]\n",
    "  r2 = XX[0,:]**2 + XX[1,:]**2\n",
    "\n",
    "  radial = 1 + np.einsum('ij,kl->il', np.tile(k,(1, N)), np.array([r2, r2**2, r2**3]))\n",
    "  tan = p[0]*XX[1,:] + p[1]*XX[0,:]\n",
    "\n",
    "  XXX = XX * np.tile(radial+tan,(2,1)) + np.outer(np.array([p[1], p[0]]).reshape(-1), r2)\n",
    "\n",
    "  Proj = (f * XXX) + c\n",
    "  Proj = Proj.T\n",
    "\n",
    "  D = X[2,]\n",
    "\n",
    "  return Proj, D, radial, tan, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_model.ipynb.\n",
      "Converted 01_loss.ipynb.\n",
      "Converted 02_skeleton.ipynb.\n",
      "Converted 03_mocap_dataset.ipynb.\n",
      "Converted 04_h36m_dataset.ipynb.\n",
      "Converted 05_camera.ipynb.\n",
      "Converted 06_quaternion.ipynb.\n",
      "Converted 07_utils.ipynb.\n",
      "Converted 08_generators.ipynb.\n",
      "Converted 09_custom_dataset.ipynb.\n",
      "Converted 10_visualization.ipynb.\n",
      "Converted 11_arguments.ipynb.\n",
      "Converted 12_data_utils.ipynb.\n",
      "Converted 13_prepare_data_2d_custom.ipynb.\n",
      "Converted 14_infer_video.ipynb.\n",
      "Converted 15_prepare_data_COCO.ipynb.\n",
      "Converted 16_pycococreatortools.ipynb.\n",
      "Converted 17_format_qtmdata.ipynb.\n",
      "Converted 18_runningpose_dataset.ipynb.\n",
      "Converted 19_train_detectron2.ipynb.\n",
      "Converted 20_transfer_model.ipynb.\n",
      "Converted 21_prepare_data_3d.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
