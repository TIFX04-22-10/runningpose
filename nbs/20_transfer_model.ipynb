{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.transfer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning temporal model \n",
    "> Trains the temporal model from the Video Pose3D checkpoint made for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from runningpose.core.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from runningpose.core.loss import mpjpe\n",
    "from runningpose.core.model import TemporalModel\n",
    "from runningpose.core.camera import normalize_screen_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "dict_keys(['Ioanna1_Camera1_170Hz_3D_keypoints', 'Ioanna2_Camera1_170Hz_3D_keypoints', 'Ioanna3_Camera1_170Hz_3D_keypoints', 'Josef1_Camera1_170Hz_3D_keypoints', 'Josef2_Camera1_170Hz_3D_keypoints', 'Josef3_Camera1_170Hz_3D_keypoints', 'Josef4_Camera1_170Hz_3D_keypoints', 'Ioanna1_Camera2_170Hz_3D_keypoints', 'Ioanna2_Camera2_170Hz_3D_keypoints', 'Ioanna3_Camera2_170Hz_3D_keypoints', 'Josef1_Camera2_170Hz_3D_keypoints', 'Josef2_Camera2_170Hz_3D_keypoints', 'Josef3_Camera2_170Hz_3D_keypoints', 'Josef4_Camera2_170Hz_3D_keypoints', 'Ioanna1_Camera3_170Hz_3D_keypoints', 'Ioanna2_Camera3_170Hz_3D_keypoints', 'Ioanna3_Camera3_170Hz_3D_keypoints', 'Josef1_Camera3_170Hz_3D_keypoints', 'Josef2_Camera3_170Hz_3D_keypoints', 'Josef3_Camera3_170Hz_3D_keypoints', 'Josef4_Camera3_170Hz_3D_keypoints'])\n",
      "(374, 18, 3)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading training dataset...')\n",
    "keypoints_3D = np.load('data_3d_train.npz', allow_pickle=True)\n",
    "keypoints_3D = keypoints_3D['positions_3d'].item()\n",
    "joints_left = [3, 6, 7, 10, 12, 14, 16]\n",
    "joints_right = [4, 8, 9, 11, 13, 15, 17]\n",
    "print(keypoints_3D.keys())\n",
    "print(keypoints_3D['Ioanna3_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a dictionary with all the videos and then a \n",
    "custom dictionary with a list with an array with (frames, keypoints, dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D training detections...\n",
      "dict_keys(['miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', 'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', 'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', 'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', 'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi', 'miqus3_Ioanna_01.avi', 'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_01.avi', 'miqus3_Josef_02.avi', 'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'])\n",
      "(323, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D = np.load('data_2d_custom_trainingdata.npz', allow_pickle=True)\n",
    "keypoints_2D_metadata = keypoints_2D['metadata'].item()\n",
    "keypoints_2D_symmetry = keypoints_2D_metadata['keypoints_symmetry']\n",
    "kps_left = list(keypoints_2D_symmetry[0])\n",
    "kps_right = list(keypoints_2D_symmetry[1])\n",
    "keypoints_2D = keypoints_2D['positions_2d'].item()\n",
    "keypoints_2D = dict(sorted(keypoints_2D.items()))\n",
    "print(keypoints_2D.keys())\n",
    "print(keypoints_2D['miqus1_Ioanna_01.avi']['custom'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize training inputs\n",
    "The resolution of the third camera is diffrent due to being cropped. \n",
    "During our datacollection we got some other people in the shot. Why we had to\n",
    "crop the video. \n",
    "\n",
    "First convert the 3D data from millimeter -> meter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for subject in keypoints_3D.keys():\n",
    "    keypoints_3D[subject] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Normalize camera frame\n",
    "subjects = [\n",
    "    'miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', \n",
    "    'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', \n",
    "    'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', \n",
    "    'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', \n",
    "    'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi'\n",
    "]\n",
    "# 2D data \n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1920, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_extra_cut = ['miqus3_Ioanna_01.avi', 'miqus3_Josef_01.avi']\n",
    "for subject in subjects_extra_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1350, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_02.avi', \n",
    "    'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "                kps = normalize_screen_coordinates(kps, w=1480, h=1088)\n",
    "                keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation dataset...\n",
      "dict_keys(['Tindra1_Camera1_170Hz_3D_keypoints', 'Tindra2_Camera1_170Hz_3D_keypoints', 'Tindra3_Camera1_170Hz_3D_keypoints', 'Tindra1_Camera2_170Hz_3D_keypoints', 'Tindra2_Camera2_170Hz_3D_keypoints', 'Tindra3_Camera2_170Hz_3D_keypoints', 'Tindra1_Camera3_170Hz_3D_keypoints', 'Tindra2_Camera3_170Hz_3D_keypoints', 'Tindra3_Camera3_170Hz_3D_keypoints'])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading validation dataset...')\n",
    "keypoints_3D_val = np.load('data_3d_val.npz', allow_pickle=True)\n",
    "keypoints_3D_val = keypoints_3D_val['positions_3d'].item()\n",
    "print(keypoints_3D_val.keys())\n",
    "# print(keypoints_3D['Ioanna1_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D training detections...\n",
      "dict_keys(['miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi', 'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi', 'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi', 'miqus3_Tindra_03.avi'])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D_val = np.load('data_2d_custom_validationdata.npz', allow_pickle=True)\n",
    "keypoints_2D_val = keypoints_2D_val['positions_2d'].item()\n",
    "keypoints_2D_val = dict(sorted(keypoints_2D_val.items()))\n",
    "print(keypoints_2D_val.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize validation input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    keypoints_3D_val[subject] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects = [\n",
    "    'miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi', \n",
    "    'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1920, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi', \n",
    "    'miqus3_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1480, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and fix errors in data\n",
    "Assert that we have the same number of frames in the 2D and 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Wrong cut .avi should have been 4.42 long but is 4.44 \n",
    "keypoints_2D['miqus3_Josef_03.avi']['custom'][0] = keypoints_2D['miqus3_Josef_03.avi']['custom'][0][:374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "shapes_3d = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    shapes_3d.append(keypoints_3D[subject].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix missing right foot values by inferring from a later frame\n",
    "> We had a missing RForefoot2 in the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera3_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna3_Camera2_170Hz_3D_keypoints'][20, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "shapes_2d = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    shapes_2d.append(keypoints_2D[subject]['custom'][0].shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for i in range(len(shapes_2d)):\n",
    "    assert shapes_2d[i][0] == shapes_3d[i][0], f'subject {i}: {shapes_2d[i][0]}, {shapes_3d[i][0]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root relative coordinates\n",
    "> The 3D predictions should be root relative so we need to convert our 3D data. WaistBack is root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in keypoints_3D.keys():\n",
    "    for i in range(keypoints_3D[subject].shape[0]):\n",
    "        keypoints_3D[subject][i, :, :] -= keypoints_3D[subject][i, 5, :]\n",
    "\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    for i in range(keypoints_3D_val[subject].shape[0]):\n",
    "        keypoints_3D_val[subject][i, :, :] -= keypoints_3D_val[subject][i, 5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roll out all the data to lists for the generators to create batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "poses_2d_train = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    poses_2d_train.append(keypoints_2D[subject]['custom'][0])\n",
    "\n",
    "poses_3d_train = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    poses_3d_train.append(keypoints_3D[subject])\n",
    "\n",
    "assert len(poses_2d_train) == len(poses_3d_train), \"Number of runs doesn't match.\"\n",
    "\n",
    "poses_2d_val = []\n",
    "for subject in keypoints_2D_val.keys():\n",
    "    poses_2d_val.append(keypoints_2D_val[subject]['custom'][0])\n",
    "\n",
    "poses_3d_val = []\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    poses_3d_val.append(keypoints_3D_val[subject])\n",
    "\n",
    "assert len(poses_2d_val) == len(poses_3d_val), \"Number of runs doesn't match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "> Load the temporal model trained model for generating 3D predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "This model was trained for 80 epochs\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('pretrained_h36m_detectron_coco.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then\n",
    "initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on\n",
    "your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up\n",
    "convergence and eliminate “hockey stick” loss curves - Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the output mean is for each keypoint over all subjects.\n",
    "allsub_keypoints = 0\n",
    "for subject in keypoints_3D.keys():\n",
    "    allsub_keypoints += np.mean(keypoints_3D[subject], axis=0)\n",
    "\n",
    "mean_keypoints = allsub_keypoints / len(keypoints_3D.keys())\n",
    "mean_keypoints = np.reshape(\n",
    "    mean_keypoints, (mean_keypoints.shape[0]*mean_keypoints.shape[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Hyperparameters\n",
    "num_joints_in = 17 # COCO\n",
    "in_features = 2 # dimension of in joints\n",
    "num_joints_out = 18\n",
    "filter_widths = [3, 3, 3, 3, 3] # just as in inference  \n",
    "causal = False # No real time predictions \n",
    "dropout = 0.25\n",
    "channels = 1024 # default\n",
    "lr = 3e-4\n",
    "lr_decay = 1\n",
    "batch_size = 256\n",
    "chunk_length = 1\n",
    "num_epochs = 10\n",
    "unfreeze_epoch = 2\n",
    "trigger_times = 0\n",
    "patience = 600 #################NOTE: Turn off\n",
    "\n",
    "# Load two models one for training and one for evaluation\n",
    "model_run_train = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "\n",
    "# Reintizialize the last output layer to fit new out. \n",
    "checkpoint['model_pos']['shrink.weight'] = torch.randn(num_joints_out*3, channels, 1)\n",
    "checkpoint['model_pos']['shrink.bias'] = torch.from_numpy(mean_keypoints)\n",
    "\n",
    "# Load the pretrained model i.e to do transfer learning\n",
    "model_run_train.load_state_dict(checkpoint['model_pos'])\n",
    "\n",
    "# Freeze all layers except the last new layer\n",
    "for name, param in model_run_train.named_parameters():\n",
    "    if name != 'shrink.weight' and name != 'shrink.bias':\n",
    "        param.requires_grad = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_run_train = model_run_train.cuda()\n",
    "    model_run = model_run.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Receptive field: 243 frames\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Calculate padding based on receptive field\n",
    "receptive_field = model_run_train.receptive_field()\n",
    "print('INFO: Receptive field: {} frames'.format(receptive_field))\n",
    "pad = (receptive_field - 1) // 2 # Padding on each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model_run_train.parameters(), lr=lr, amsgrad=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# Initialize loss\n",
    "losses_3d_train = []\n",
    "losses_3d_train_eval = []\n",
    "losses_3d_valid = []\n",
    "\n",
    "# Using batch norm momentum\n",
    "initial_momentum = 0.1\n",
    "final_momentum = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Testing on 2414 frames\n",
      "INFO: Training on 6426 frames\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "valid_generator = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_val, poses_2d=poses_2d_val,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "print('INFO: Testing on {} frames'.format(valid_generator.num_frames()))\n",
    "\n",
    "train_generator = ChunkedGenerator(\n",
    "    batch_size, cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, chunk_length=chunk_length, shuffle=True, augment=True, \n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "train_generator_eval = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, augment=False\n",
    ")\n",
    "print('INFO: Training on {} frames'.format(train_generator_eval.num_frames()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Trains with freezed layers first then moves on training the whole net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/autocast_mode.py:162: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/media/richard/SecondLinux/repos/runningpose/nbs/20_transfer_model.ipynb Cell 42'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/richard/SecondLinux/repos/runningpose/nbs/20_transfer_model.ipynb#ch0000039?line=25'>26</a>\u001b[0m \u001b[39m# Predict 3D poses (forward) using fp16\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/richard/SecondLinux/repos/runningpose/nbs/20_transfer_model.ipynb#ch0000039?line=26'>27</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/media/richard/SecondLinux/repos/runningpose/nbs/20_transfer_model.ipynb#ch0000039?line=27'>28</a>\u001b[0m     predicted_3d_pos \u001b[39m=\u001b[39m model_run_train(inputs_2d)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/richard/SecondLinux/repos/runningpose/nbs/20_transfer_model.ipynb#ch0000039?line=28'>29</a>\u001b[0m     loss_3d_pos \u001b[39m=\u001b[39m mpjpe(predicted_3d_pos, inputs_3d)\n\u001b[1;32m     <a href='vscode-notebook-cell:/media/richard/SecondLinux/repos/runningpose/nbs/20_transfer_model.ipynb#ch0000039?line=29'>30</a>\u001b[0m     N \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m inputs_3d\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py:74\u001b[0m, in \u001b[0;36mTemporalModelBase.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=70'>71</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=71'>72</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=73'>74</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_blocks(x)\n\u001b[1;32m     <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=75'>76</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=76'>77</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(sz[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_joints_out, \u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m/media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py:147\u001b[0m, in \u001b[0;36mTemporalModel._forward_blocks\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=144'>145</a>\u001b[0m     shift \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcausal_shift[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=145'>146</a>\u001b[0m     res \u001b[39m=\u001b[39m x[:, :, pad \u001b[39m+\u001b[39m shift : x\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m pad \u001b[39m+\u001b[39m shift]\n\u001b[0;32m--> <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=146'>147</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdrop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\n\u001b[1;32m    <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=147'>148</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers_bn[\u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49mi](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayers_conv[\u001b[39m2\u001b[39;49m\u001b[39m*\u001b[39;49mi](x))))\n\u001b[1;32m    <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=148'>149</a>\u001b[0m     x \u001b[39m=\u001b[39m res \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\n\u001b[1;32m    <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=149'>150</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers_bn[\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers_conv[\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m](x))))\n\u001b[1;32m    <a href='file:///media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/model.py?line=151'>152</a>\u001b[0m \u001b[39m# Fits the last layer so that it matches our output preferences.\u001b[39;00m\n",
      "File \u001b[0;32m/media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/dropout.py?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/modules/dropout.py?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m/media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/functional.py:1279\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/functional.py?line=1276'>1277</a>\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/functional.py?line=1277'>1278</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> <a href='file:///media/richard/SecondLinux/minconda3/envs/ffmpeg/lib/python3.10/site-packages/torch/nn/functional.py?line=1278'>1279</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#export\n",
    "epoch = 0\n",
    "while epoch < num_epochs:\n",
    "    start_time = time.time()\n",
    "    # Unfreezes the layers after a given epoch\n",
    "    if epoch == unfreeze_epoch:\n",
    "            print('Unfreezes the model after epoch:', epoch)\n",
    "            for param in model_run_train.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    # Initialize training loss\n",
    "    epoch_loss_3d_train = 0\n",
    "    epoch_loss_2d_train_unlabeled = 0\n",
    "    N = 0\n",
    "    # Regular supervised scenario\n",
    "    for _, batch_3d, batch_2d in train_generator.next_epoch():\n",
    "        inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "        inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_3d = inputs_3d.cuda()\n",
    "            inputs_2d = inputs_2d.cuda()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predict 3D poses (forward) using fp16\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predicted_3d_pos = model_run_train(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss_3d_pos).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    # Total loss over one epoch\n",
    "    losses_3d_train.append(epoch_loss_3d_train / N)\n",
    "    \n",
    "    # End-of-epoch evaluation\n",
    "    with torch.no_grad():\n",
    "        # Load the newly trained network\n",
    "        model_run.load_state_dict(model_run_train.state_dict())\n",
    "        model_run.eval()\n",
    "        # Initialize validation loss\n",
    "        epoch_loss_3d_valid = 0\n",
    "        epoch_loss_2d_valid = 0\n",
    "        N = 0\n",
    "\n",
    "        # Evaluate on validation dataset\n",
    "        for _, batch_3d, batch_2d in valid_generator.next_epoch():\n",
    "            inputs_3d_valid = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d_valid = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d_valid = inputs_3d_valid.cuda()\n",
    "                inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d_valid)\n",
    "            N += inputs_3d_valid.shape[0]\n",
    "            epoch_loss_3d_valid += inputs_3d_valid.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_valid.append(epoch_loss_3d_valid / N)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch > 1:\n",
    "            if losses_3d_valid[-1] > losses_3d_valid[-2]:\n",
    "                trigger_times += 1\n",
    "                print('Trigger Times:', trigger_times, flush=True)\n",
    "\n",
    "                if trigger_times > patience:\n",
    "                    print('Early stopping! at epoch:', epoch+1)\n",
    "                    epoch = num_epochs\n",
    "                    \n",
    "        # Evaluate on training set, this time in evaluation mode\n",
    "        epoch_loss_3d_train_eval = 0\n",
    "        epoch_loss_2d_train_labeled_eval = 0\n",
    "        N = 0\n",
    "        for _, batch_3d, batch_2d in train_generator_eval.next_epoch():\n",
    "            if batch_2d.shape[1] == 0:\n",
    "                # This can only happen when downsampling the dataset\n",
    "                continue\n",
    "            \n",
    "            inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d = inputs_3d.cuda()\n",
    "                inputs_2d = inputs_2d.cuda()\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train_eval += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_train_eval.append(epoch_loss_3d_train_eval / N)\n",
    "\n",
    "    # Calculate total training/validation time over one epoch       \n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f'''[{epoch+1}] time {elapsed:.2f} lr {lr} \n",
    "        3d_train {losses_3d_train[-1] * 1000} \n",
    "        3d_eval {losses_3d_train_eval[-1] * 1000} \n",
    "        3d_valid {losses_3d_valid[-1]  * 1000}''', \n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    # Decay learning rate exponentially\n",
    "    lr *= lr_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    epoch += 1\n",
    "\n",
    "    # Decay BatchNorm momentum\n",
    "    momentum = initial_momentum * np.exp(\n",
    "        -epoch/num_epochs * np.log(initial_momentum/final_momentum)\n",
    "    )\n",
    "    model_run_train.set_bn_momentum(momentum)\n",
    "\n",
    "    # Save training curves after every epoch, as .png images\n",
    "    if epoch >= num_epochs:\n",
    "        plt.figure()\n",
    "        epoch_x = np.arange(5, len(losses_3d_train)) + 1\n",
    "        plt.plot(epoch_x, losses_3d_train[5:], '--', color='C0')\n",
    "        plt.plot(epoch_x, losses_3d_train_eval[5:], color='C0')\n",
    "        plt.plot(epoch_x, losses_3d_valid[5:], color='C1')\n",
    "        plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])\n",
    "        plt.ylabel('MPJPE (m)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((3, epoch))\n",
    "        plt.savefig('loss_plots/' + str(epoch) + '_loss_3d.png')\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "chk_path = os.path.join('runningpose_epoch_{}.bin'.format(epoch))\n",
    "print(chk_path)\n",
    "print('Saving checkpoint to', chk_path)\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'lr': lr,\n",
    "    'random_state': train_generator.random_state(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_run': model_run_train.state_dict(),\n",
    "}, chk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualize the predictions on one of our validation videos\n",
    "> This is highly temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "This model was trained for 100 epochs\n",
      "Rendering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3.2 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 10.3.0 (GCC)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1645955405450/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1645955405450/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1645955405450/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, avi, from 'Josef_01_Miqus_14.avi':\n",
      "  Metadata:\n",
      "    encoder         : Lavf59.16.100\n",
      "  Duration: 00:00:02.81, start: 0.000000, bitrate: 106362 kb/s\n",
      "    Stream #0:0: Video: mjpeg (Baseline) (MJPG / 0x47504A4D), yuvj420p(pc, bt470bg/unknown/unknown), 1920x1088, 106777 kb/s, 85 fps, 85 tbr, 85 tbn, 85 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> rawvideo (native))\n",
      "Press [q] to stop, [?] for help\n",
      "[swscaler @ 0x55a23783e200] deprecated pixel format used, make sure you did set range correctly\n",
      "Output #0, image2pipe, to 'pipe:':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0: Video: rawvideo (RGB[24] / 0x18424752), rgb24, 1920x1088, q=2-31, 4261478 kb/s, 85 fps, 85 tbn, 85 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.91.100 rawvideo\n",
      "frame=  238 fps=143 q=-0.0 Lsize= 1456560kB time=00:00:02.81 bitrate=4243647.5kbits/s speed=1.69x    \n",
      "video:1456560kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\n",
      "/media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/visualization.py:278: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237/238\r"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from runningpose.core.skeleton import Skeleton\n",
    "runningpose_skeleton = Skeleton(\n",
    "    parents=[-1, 0, 1, 1, 1, 2, 5, 16, 5, 17, 3, 4, 10, 11, 6, 8, 14, 15],\n",
    "    joints_left=[3, 6, 7, 10, 12, 14, 16], \n",
    "    joints_right=[4, 8, 9, 11, 13, 15, 17]\n",
    ")\n",
    "h36m_skeleton = Skeleton(\n",
    "       parents=[-1, 0, 1, 2, 3, 4, 0, 6, 7, 8],\n",
    "       joints_left=[6, 7, 8, 9, 10],\n",
    "       joints_right=[1, 2, 3, 4, 5]\n",
    ")\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('runningpose_overtuned.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model_run = model_run.cuda()\n",
    "\n",
    "model_run.load_state_dict(checkpoint['model_run'])\n",
    "\n",
    "from runningpose.core.runningpose_dataset import runningpose_cameras_extrinsic_params\n",
    "from runningpose.core.camera import camera_to_world_miqus, image_coordinates\n",
    "from runningpose.core.visualization import render_animation\n",
    "tindra_cam1 = keypoints_2D_val['miqus1_Tindra_01.avi']['custom']\n",
    "josef_cam1 = keypoints_2D['miqus1_Josef_01.avi']['custom']\n",
    "gen = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=None, poses_2d=josef_cam1,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "predicted = 0\n",
    "data_world = 0\n",
    "print('Rendering...')\n",
    "with torch.no_grad():\n",
    "    # Load the newly trained network\n",
    "    model_run.eval()\n",
    "\n",
    "    for _, _, batch2d in gen.next_epoch():\n",
    "        inputs_2d_valid = torch.from_numpy(batch2d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "\n",
    "        # Predict 3D poses (forward)\n",
    "        predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "        # Convert predicted 3d poses to world coordinates. \n",
    "        predicted_3d_pos = predicted_3d_pos.cpu().detach().numpy()[0]\n",
    "        predicted = predicted_3d_pos\n",
    "        predicted_3d_pos = predicted_3d_pos.transpose(1, 0, 2)\n",
    "    \n",
    "        # Get camera parameters.\n",
    "        R = runningpose_cameras_extrinsic_params[0]['rotation']\n",
    "        T = 0 \n",
    "        data_3D_world = []\n",
    "        for keypoint in predicted_3d_pos:\n",
    "            data_3D_world.append(camera_to_world_miqus(keypoint, R, T))\n",
    "\n",
    "        data_3D_world = np.array(data_3D_world).transpose(1, 0, 2)\n",
    "        data_3D_world[:, :, 2] -= np.min(data_3D_world[:, :, 2])\n",
    "        data_world = data_3D_world\n",
    "        \n",
    "        anim_output = {'Reconstruction': data_3D_world}\n",
    "        input_keypoints = image_coordinates(josef_cam1[0][..., :2], w=1920, h=1088)\n",
    "        \n",
    "        render_animation(\n",
    "            input_keypoints, keypoints_2D_metadata, anim_output,\n",
    "            runningpose_skeleton, 85, 3000, 70, 'josef01_cam1.gif', size=4, \n",
    "            input_video_path='Josef_01_Miqus_14.avi', viewport=(1920, 1088)\n",
    "        )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42303429,  0.26015752,  0.45581345],\n",
       "       [ 0.25422998,  0.29278402,  0.2693365 ],\n",
       "       [ 0.08921538,  0.11156386,  0.06648251],\n",
       "       [ 0.36620989,  0.25680363,  0.21468882],\n",
       "       [ 0.14327012,  0.25385077,  0.37796204],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.13445541, -0.11719234, -0.11481342],\n",
       "       [-0.30846427, -0.73945383, -0.50701398],\n",
       "       [-0.03658132, -0.11472183,  0.17688657],\n",
       "       [ 0.10396421, -0.73026476,  0.16168209],\n",
       "       [ 0.56788958,  0.02446951,  0.20959372],\n",
       "       [-0.09573019,  0.33995899,  0.22350433],\n",
       "       [ 0.57400993,  0.19410068,  0.37322601],\n",
       "       [-0.32357198,  0.29836445,  0.10136282],\n",
       "       [-0.08741295, -0.41563311, -0.23919829],\n",
       "       [ 0.22689401, -0.33543887,  0.23222618],\n",
       "       [-0.30944753, -0.6124015 , -0.48556912],\n",
       "       [ 0.06312181, -0.62748536,  0.091108  ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = keypoints_3D['Josef1_Camera1_170Hz_3D_keypoints']\n",
    "input[-1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "josef_cam1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.51457702e-01, -5.55888148e-02,  1.24585441e+00],\n",
       "       [ 3.19046907e-01,  8.70417099e-02,  1.28859515e+00],\n",
       "       [ 1.32917882e-01,  1.18994460e-01,  9.90692651e-01],\n",
       "       [ 4.00851926e-01,  7.67496207e-02,  1.25796740e+00],\n",
       "       [ 3.34549760e-01, -1.26451053e-01,  1.32418406e+00],\n",
       "       [-1.26689397e-02,  4.75715769e-02,  1.04305712e+00],\n",
       "       [ 4.17065565e-02,  1.44956873e-01,  9.39580740e-01],\n",
       "       [-5.24454412e-01,  4.79791096e-02,  1.75782697e-01],\n",
       "       [ 1.17671038e-01, -2.03634409e-01,  7.86256589e-01],\n",
       "       [ 2.34132135e-01, -1.20842517e-01,  2.94283479e-01],\n",
       "       [ 4.73213489e-01,  2.01231589e-01,  9.79725236e-01],\n",
       "       [ 1.43376905e-01, -3.41055914e-01,  1.20111383e+00],\n",
       "       [ 7.03693740e-01, -1.18819063e-03,  1.16567796e+00],\n",
       "       [-2.42053273e-01, -3.30895073e-01,  1.20453803e+00],\n",
       "       [-1.75004399e-01,  1.64514874e-01,  6.09590697e-01],\n",
       "       [ 2.80531076e-01, -9.55331498e-02,  6.50585727e-01],\n",
       "       [-4.64756086e-01,  8.89108347e-02,  3.79055026e-01],\n",
       "       [ 1.40212465e-01,  6.23970851e-02,  5.31073513e-01]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_world[-1, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.37269884,  0.21561809,  0.44779631],\n",
       "       [ 0.29915917,  0.2947046 ,  0.19378416],\n",
       "       [ 0.17879465,  0.02178069,  0.00554004],\n",
       "       [ 0.3518138 ,  0.2549355 ,  0.2519387 ],\n",
       "       [ 0.16653976,  0.30958998,  0.36488825],\n",
       "       [ 0.02425397,  0.0825498 , -0.03271024],\n",
       "       [ 0.12842864, -0.01707054, -0.08140721],\n",
       "       [-0.36677918, -0.72327644, -0.49295676],\n",
       "       [-0.05451431, -0.20750384,  0.20330626],\n",
       "       [ 0.07872485, -0.69974667,  0.15432128],\n",
       "       [ 0.48455584, -0.01731333,  0.17187122],\n",
       "       [-0.12155071,  0.18873145,  0.37696043],\n",
       "       [ 0.52011716,  0.12510826,  0.49917048],\n",
       "       [-0.39803568,  0.23317257,  0.11190882],\n",
       "       [-0.02337512, -0.31968534, -0.28541583],\n",
       "       [ 0.13613904, -0.34937933,  0.21507534],\n",
       "       [-0.29164234, -0.52452433, -0.45539355],\n",
       "       [ 0.13788489, -0.43928245, -0.01037715]], dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[-1, :, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
