{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.transfer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning temporal model \n",
    "> Trains the temporal model from the Video Pose3D checkpoint made for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "\n",
    "from runningpose.core.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from runningpose.core.loss import mpjpe\n",
    "from runningpose.core.model import TemporalModel\n",
    "from runningpose.core.runningpose_dataset import RunningposeDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# print('Loading dataset...')\n",
    "# dataset_path = ''\n",
    "# dataset = RunningposeDataset(dataset_path)\n",
    "\n",
    "# Convert to 3D camera coordinates \n",
    "# TODO: Maybe just do this in format qtmdata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D detections...\n",
      "dict_keys(['Tindra_dynamic0001_Miqus_16_23604.avi', 'Tindra_dynamic0001_Miqus_15_21386.avi'])\n",
      "dict_keys(['custom'])\n",
      "(694, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D detections...')\n",
    "keypoints = np.load('data_2d_custom_tindra.npz', allow_pickle=True)\n",
    "keypoints_metadata = keypoints['metadata'].item()\n",
    "keypoints_symmetry = keypoints_metadata['keypoints_symmetry']\n",
    "kps_left, kps_right = list(keypoints_symmetry[0]), list(keypoints_symmetry[1])\n",
    "# joints_left, joints_right = list(dataset.skeleton().joints_left()), list(dataset.skeleton().joints_right())\n",
    "keypoints = keypoints['positions_2d'].item()\n",
    "print(keypoints.keys())\n",
    "print(keypoints['Tindra_dynamic0001_Miqus_16_23604.avi'].keys())\n",
    "print(keypoints['Tindra_dynamic0001_Miqus_16_23604.avi']['custom'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a dictionary with all the videos and then a \n",
    "custom dictionary with a list with an array with (frames, keypoints, dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "> Load the temporal model trained model for generating 3D predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "This model was trained for 80 epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'lr', 'model_pos'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('pretrained_h36m_detectron_coco.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "# Hyperparameters\n",
    "num_joints_in = 17 # COCO\n",
    "in_features = 2 # dimension of in joints\n",
    "num_joints_out = 20 # runningpose\n",
    "filter_widths = [3, 3, 3, 3, 3] # just as in inference  \n",
    "causal = False # No real time predictions \n",
    "dropout = 0.25 # default\n",
    "channels = 1024 # default\n",
    "lr = 0.001 # default\n",
    "lr_decay = 0.95 # default\n",
    "batch_size = 64\n",
    "chunk_length = 1\n",
    "num_epochs = 10\n",
    "\n",
    "# Load two models one for training and one for evaluation\n",
    "model_run_train = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_run_train = model_run_train.cuda()\n",
    "    model_run = model_run.cuda()\n",
    "\n",
    "model_run_train.load_state_dict(checkpoint['model_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Receptive field: 243 frames\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Calculate padding based on receptive field\n",
    "receptive_field = model_run_train.receptive_field()\n",
    "print('INFO: Receptive field: {} frames'.format(receptive_field))\n",
    "pad = (receptive_field - 1) // 2 # Padding on each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model_run_train.parameters(), lr=lr, amsgrad=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Initialize loss\n",
    "losses_3d_train = []\n",
    "losses_3d_train_eval = []\n",
    "losses_3d_valid = []\n",
    "\n",
    "# Using batch norm momentum\n",
    "initial_momentum = 0.1\n",
    "final_momentum = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3869/3210694694.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcameras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes_3d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mchunk_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkps_left\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkps_left\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mkps_right\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkps_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoints_left\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoints_right\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/generators.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, cameras, poses_3d, poses_2d, chunk_length, pad, causal_shift, shuffle, random_seed, augment, kps_left, kps_right, joints_left, joints_right, endless)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Build lineage info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# (seq_idx, start_frame, end_frame, flip) tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposes_2d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mn_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mposes_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mchunk_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_chunks\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mchunk_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mposes_2d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "#export\n",
    "valid_generator = UnchunkedGenerator(\n",
    "    cameras_valid=None, poses_valid=None, poses_valid_2d=None,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=None, joints_right=None\n",
    ")\n",
    "print('INFO: Testing on {} frames'.format(valid_generator.num_frames()))\n",
    "train_generator = ChunkedGenerator(\n",
    "    batch_size, cameras=None, poses_3d=None, poses_2d=None, \n",
    "    chunk_length=chunk_length, shuffle=True, augment=False, kps_left=kps_left, \n",
    "    kps_right=kps_right, joints_left=None, joints_right=None\n",
    ")\n",
    "train_generator_eval = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=None, poses_2d=None, pad=pad, augment=False\n",
    ")\n",
    "print('INFO: Training on {} frames'.format(train_generator_eval.num_frames()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "epoch = 0\n",
    "while epoch < num_epochs:\n",
    "    start_time = time()\n",
    "    # Initialize training loss\n",
    "    epoch_loss_3d_train = 0\n",
    "    epoch_loss_traj_train = 0\n",
    "    epoch_loss_2d_train_unlabeled = 0\n",
    "    N = 0\n",
    "    # Regular supervised scenario\n",
    "    for _, batch_3d, batch_2d in train_generator.next_expoch():\n",
    "        inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "        inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_3d = inputs_3d.cuda()\n",
    "            inputs_2d = inputs_2d.cuda()\n",
    "        inputs_3d[:, :, 0] = 0\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict 3D poses (forward)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predicted_3d_pos = model_run_train(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]*inputs_3d.shape[1]\n",
    "            # Calculate the epoch loss\n",
    "            epoch_loss_3d_train += inputs_3d.shape[0] \\\n",
    "            * inputs_3d.shape[1] * loss_3d_pos.item()\n",
    "\n",
    "        # Backward\n",
    "        loss_total = loss_3d_pos\n",
    "        scaler.scale(loss_total).backward()\n",
    "        scaler.step(optimizer)\n",
    "        optimizer.update()\n",
    "\n",
    "    # Total loss over one epoch\n",
    "    losses_3d_train.append(epoch_loss_3d_train / N)\n",
    "    \n",
    "    # End-of-epoch evaluation\n",
    "    with torch.no_grad():\n",
    "        # Load the newly trained network\n",
    "        model_run.load_state_dict(model_run_train.state_dict())\n",
    "        model_run.eval()\n",
    "        # Initialize validation loss\n",
    "        epoch_loss_3d_valid = 0\n",
    "        epoch_loss_traj_valid = 0\n",
    "        epoch_loss_2d_valid = 0\n",
    "        N = 0\n",
    "\n",
    "        # Evaluate on validation dataset\n",
    "        for _, batch_3d, batch_2d in valid_generator.next_epoch():\n",
    "            inputs_3d_valid = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d_valid = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d_valid = inputs_3d_valid.cuda()\n",
    "                inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "            inputs_3d_valid[:, :, 0] = 0\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d_valid)\n",
    "            N += inputs_3d_valid.shape[0]*inputs_3d_valid.shape[1]\n",
    "            epoch_loss_3d_valid += inputs_3d_valid.shape[0] \\\n",
    "            * inputs_3d_valid.shape[1] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_valid.append(epoch_loss_3d_valid / N)\n",
    "\n",
    "        # Evaluate on training set, this time in evaluation mode\n",
    "        epoch_loss_3d_train_eval = 0\n",
    "        epoch_loss_traj_train_eval = 0\n",
    "        epoch_loss_2d_train_labeled_eval = 0\n",
    "        N = 0\n",
    "        for _, batch_3d, batch_2d in train_generator_eval.next_epoch():\n",
    "            if batch_2d.shape[1] == 0:\n",
    "                # This can only happen when downsampling the dataset\n",
    "                continue\n",
    "            \n",
    "            inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d = inputs_3d.cuda()\n",
    "                inputs_2d = inputs_2d.cuda()\n",
    "            inputs_3d[:, :, 0] = 0\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]*inputs_3d.shape[1]\n",
    "            epoch_loss_3d_valid += inputs_3d.shape[0] \\\n",
    "            * inputs_3d.shape[1] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_train_eval.append(epoch_loss_3d_train_eval / N)\n",
    "\n",
    "    # Calculate total training/validation time over one epoch       \n",
    "    elapsed = (time() - start_time)/60\n",
    "\n",
    "    print(\n",
    "        f'''[{epoch+1}] time {elapsed:.2f} lr {lr} \n",
    "        3d_train {losses_3d_train[-1] * 1000} \n",
    "        3d_eval {losses_3d_train_eval[-1] * 1000} \n",
    "        3d_valid {losses_3d_valid[-1]  *1000}'''\n",
    "    )\n",
    "\n",
    "    # Decay learning rate exponentially\n",
    "    lr *= lr_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    epoch += 1\n",
    "\n",
    "    # Decay BatchNorm momentum\n",
    "    momentum = initial_momentum * np.exp(\n",
    "        -epoch/num_epochs * np.log(initial_momentum/final_momentum)\n",
    "    )\n",
    "    model_run_train.set_bn_momentum(momentum)\n",
    "\n",
    "    # Save training curves after every epoch, as .png images\n",
    "    plt.figure()\n",
    "    epoch_x = np.arange(3, len(losses_3d_train)) + 1\n",
    "    plt.plot(epoch_x, losses_3d_train[3:], '--', color='C0')\n",
    "    plt.plot(epoch_x, losses_3d_train_eval[3:], color='C0')\n",
    "    plt.plot(epoch_x, losses_3d_valid[3:], color='C1')\n",
    "    plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])\n",
    "    plt.ylabel('MPJPE (m)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim((3, epoch))\n",
    "    plt.savefig(os.path.join(str(epoch), '_loss_3d.png'))\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = os.path.join('runningpose', '_epoch_{}.bin'.format(epoch))\n",
    "print('Saving checkpoint to', chk_path)\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'lr': lr,\n",
    "    'random_state': train_generator.random_state(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_run': model_run_train.state_dict(),\n",
    "}, chk_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
