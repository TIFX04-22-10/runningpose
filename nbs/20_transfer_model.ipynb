{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.transfer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning temporal model \n",
    "> Trains the temporal model from the Video Pose3D checkpoint made for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from runningpose.core.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from runningpose.core.loss import mpjpe\n",
    "from runningpose.core.model import TemporalModel\n",
    "from runningpose.core.camera import normalize_screen_coordinates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "dict_keys(['Ioanna1_Camera1_170Hz_3D_keypoints', 'Ioanna2_Camera1_170Hz_3D_keypoints', 'Ioanna3_Camera1_170Hz_3D_keypoints', 'Josef1_Camera1_170Hz_3D_keypoints', 'Josef2_Camera1_170Hz_3D_keypoints', 'Josef3_Camera1_170Hz_3D_keypoints', 'Josef4_Camera1_170Hz_3D_keypoints', 'Ioanna1_Camera2_170Hz_3D_keypoints', 'Ioanna2_Camera2_170Hz_3D_keypoints', 'Ioanna3_Camera2_170Hz_3D_keypoints', 'Josef1_Camera2_170Hz_3D_keypoints', 'Josef2_Camera2_170Hz_3D_keypoints', 'Josef3_Camera2_170Hz_3D_keypoints', 'Josef4_Camera2_170Hz_3D_keypoints', 'Ioanna1_Camera3_170Hz_3D_keypoints', 'Ioanna2_Camera3_170Hz_3D_keypoints', 'Ioanna3_Camera3_170Hz_3D_keypoints', 'Josef1_Camera3_170Hz_3D_keypoints', 'Josef2_Camera3_170Hz_3D_keypoints', 'Josef3_Camera3_170Hz_3D_keypoints', 'Josef4_Camera3_170Hz_3D_keypoints'])\n",
      "(374, 18, 3)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading training dataset...')\n",
    "keypoints_3D = np.load('data_3d_train.npz', allow_pickle=True)\n",
    "keypoints_3D = keypoints_3D['positions_3d'].item()\n",
    "joints_left = [1, 3, 5, 7, 10, 11, 14]\n",
    "joints_right = [0, 2, 4, 6, 8, 9, 13]\n",
    "print(keypoints_3D.keys())\n",
    "print(keypoints_3D['Ioanna3_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a dictionary with all the videos and then a \n",
    "custom dictionary with a list with an array with (frames, keypoints, dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D training detections...\n",
      "dict_keys(['miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', 'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', 'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', 'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', 'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi', 'miqus3_Ioanna_01.avi', 'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_01.avi', 'miqus3_Josef_02.avi', 'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'])\n",
      "(323, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D = np.load('data_2d_custom_trainingdata.npz', allow_pickle=True)\n",
    "keypoints_2D_metadata = keypoints_2D['metadata'].item()\n",
    "keypoints_2D_symmetry = keypoints_2D_metadata['keypoints_symmetry']\n",
    "kps_left = list(keypoints_2D_symmetry[0])\n",
    "kps_right = list(keypoints_2D_symmetry[1])\n",
    "keypoints_2D = keypoints_2D['positions_2d'].item()\n",
    "keypoints_2D = dict(sorted(keypoints_2D.items()))\n",
    "print(keypoints_2D.keys())\n",
    "print(keypoints_2D['miqus1_Ioanna_01.avi']['custom'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize training inputs\n",
    "The resolution of the third camera is diffrent due to being cropped. \n",
    "During our datacollection we got some other people in the shot. Why we had to\n",
    "crop the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Normalize camera frame\n",
    "subjects = [\n",
    "    'miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', \n",
    "    'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', \n",
    "    'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', \n",
    "    'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', \n",
    "    'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi'\n",
    "]\n",
    "# 2D data \n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1920, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_extra_cut = ['miqus3_Ioanna_01.avi', 'miqus3_Josef_01.avi']\n",
    "for subject in subjects_extra_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1350, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_02.avi', \n",
    "    'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "                kps = normalize_screen_coordinates(kps, w=1480, h=1088)\n",
    "                keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation dataset...\n",
      "dict_keys(['Tindra1_Camera1_170Hz_3D_keypoints', 'Tindra2_Camera1_170Hz_3D_keypoints', 'Tindra3_Camera1_170Hz_3D_keypoints', 'Tindra1_Camera2_170Hz_3D_keypoints', 'Tindra2_Camera2_170Hz_3D_keypoints', 'Tindra3_Camera2_170Hz_3D_keypoints', 'Tindra1_Camera3_170Hz_3D_keypoints', 'Tindra2_Camera3_170Hz_3D_keypoints', 'Tindra3_Camera3_170Hz_3D_keypoints'])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading validation dataset...')\n",
    "keypoints_3D_val = np.load('data_3d_val.npz', allow_pickle=True)\n",
    "keypoints_3D_val = keypoints_3D_val['positions_3d'].item()\n",
    "print(keypoints_3D_val.keys())\n",
    "# print(keypoints_3D['Ioanna1_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D training detections...\n",
      "dict_keys(['miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi', 'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi', 'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi', 'miqus3_Tindra_03.avi'])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D_val = np.load('data_2d_custom_validationdata.npz', allow_pickle=True)\n",
    "keypoints_2D_val = keypoints_2D_val['positions_2d'].item()\n",
    "keypoints_2D_val = dict(sorted(keypoints_2D_val.items()))\n",
    "print(keypoints_2D_val.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize validation input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects = [\n",
    "    'miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi', \n",
    "    'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1920, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi', \n",
    "    'miqus3_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1480, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assert that we have the same number of frames in the 2D and 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Wrong cut .avi should have been 4.42 long but is 4.44 \n",
    "keypoints_2D['miqus3_Josef_03.avi']['custom'][0] = keypoints_2D['miqus3_Josef_03.avi']['custom'][0][:374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Ioanna1_Camera1_170Hz_3D_keypoints\n",
      "True Ioanna2_Camera1_170Hz_3D_keypoints\n",
      "False Ioanna3_Camera1_170Hz_3D_keypoints\n",
      "False Josef1_Camera1_170Hz_3D_keypoints\n",
      "False Josef2_Camera1_170Hz_3D_keypoints\n",
      "False Josef3_Camera1_170Hz_3D_keypoints\n",
      "False Josef4_Camera1_170Hz_3D_keypoints\n",
      "False Ioanna1_Camera2_170Hz_3D_keypoints\n",
      "True Ioanna2_Camera2_170Hz_3D_keypoints\n",
      "False Ioanna3_Camera2_170Hz_3D_keypoints\n",
      "False Josef1_Camera2_170Hz_3D_keypoints\n",
      "False Josef2_Camera2_170Hz_3D_keypoints\n",
      "False Josef3_Camera2_170Hz_3D_keypoints\n",
      "False Josef4_Camera2_170Hz_3D_keypoints\n",
      "False Ioanna1_Camera3_170Hz_3D_keypoints\n",
      "True Ioanna2_Camera3_170Hz_3D_keypoints\n",
      "False Ioanna3_Camera3_170Hz_3D_keypoints\n",
      "False Josef1_Camera3_170Hz_3D_keypoints\n",
      "False Josef2_Camera3_170Hz_3D_keypoints\n",
      "False Josef3_Camera3_170Hz_3D_keypoints\n",
      "False Josef4_Camera3_170Hz_3D_keypoints\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "shapes_3d = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    shapes_3d.append(keypoints_3D[subject].shape)\n",
    "    print(np.isnan(keypoints_3D[subject]).any(), subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix missing right foot values by inferring from a later frame\n",
    "> We had a missing RForefoot2 in the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera3_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna3_Camera2_170Hz_3D_keypoints'][20, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False miqus1_Ioanna_01.avi\n",
      "False miqus1_Ioanna_02.avi\n",
      "False miqus1_Ioanna_03.avi\n",
      "False miqus1_Josef_01.avi\n",
      "False miqus1_Josef_02.avi\n",
      "False miqus1_Josef_03.avi\n",
      "False miqus1_Josef_04.avi\n",
      "False miqus2_Ioanna_01.avi\n",
      "False miqus2_Ioanna_02.avi\n",
      "False miqus2_Ioanna_03.avi\n",
      "False miqus2_Josef_01.avi\n",
      "False miqus2_Josef_02.avi\n",
      "False miqus2_Josef_03.avi\n",
      "False miqus2_Josef_04.avi\n",
      "False miqus3_Ioanna_01.avi\n",
      "False miqus3_Ioanna_02.avi\n",
      "False miqus3_Ioanna_03.avi\n",
      "False miqus3_Josef_01.avi\n",
      "False miqus3_Josef_02.avi\n",
      "False miqus3_Josef_03.avi\n",
      "False miqus3_Josef_04.avi\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "shapes_2d = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    shapes_2d.append(keypoints_2D[subject]['custom'][0].shape)\n",
    "    print(np.isnan(keypoints_2D[subject]['custom'][0]).any(), subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for i in range(len(shapes_2d)):\n",
    "    assert shapes_2d[i][0] == shapes_3d[i][0], f'subject {i}: {shapes_2d[i][0]}, {shapes_3d[i][0]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roll out all the data to lists for the generators to create batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "poses_2d_train = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    poses_2d_train.append(keypoints_2D[subject]['custom'][0])\n",
    "\n",
    "poses_3d_train = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    poses_3d_train.append(keypoints_3D[subject])\n",
    "\n",
    "assert len(poses_2d_train) == len(poses_3d_train), \"Number of runs doesn't match.\"\n",
    "\n",
    "poses_2d_val = []\n",
    "for subject in keypoints_2D_val.keys():\n",
    "    poses_2d_val.append(keypoints_2D_val[subject]['custom'][0])\n",
    "\n",
    "poses_3d_val = []\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    poses_3d_val.append(keypoints_3D_val[subject])\n",
    "\n",
    "assert len(poses_2d_val) == len(poses_3d_val), \"Number of runs doesn't match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "> Load the temporal model trained model for generating 3D predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "This model was trained for 80 epochs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'lr', 'model_pos'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('pretrained_h36m_detectron_coco.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "# Hyperparameters\n",
    "num_joints_in = 17 # COCO\n",
    "in_features = 2 # dimension of in joints\n",
    "num_joints_out = 18\n",
    "\n",
    " # runningpose\n",
    "filter_widths = [3, 3, 3, 3, 3] # just as in inference  \n",
    "causal = False # No real time predictions \n",
    "dropout = 0.25 # default\n",
    "channels = 1024 # default\n",
    "lr = 0.001 # default\n",
    "lr_decay = 0.95 # default\n",
    "batch_size = 256\n",
    "chunk_length = 1\n",
    "num_epochs = 2\n",
    "\n",
    "# Load two models one for training and one for evaluation\n",
    "model_run_train = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_run_train = model_run_train.cuda()\n",
    "    model_run = model_run.cuda()\n",
    "\n",
    "# Reintizialize the last output layer to fit new out. \n",
    "checkpoint['model_pos']['shrink.weight'] = torch.randn(num_joints_out*3, channels, 1)\n",
    "checkpoint['model_pos']['shrink.bias'] = torch.randn(num_joints_out*3)\n",
    "\n",
    "# Load the pretrained model i.e to do transfer learning\n",
    "model_run_train.load_state_dict(checkpoint['model_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Receptive field: 243 frames\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Calculate padding based on receptive field\n",
    "receptive_field = model_run_train.receptive_field()\n",
    "print('INFO: Receptive field: {} frames'.format(receptive_field))\n",
    "pad = (receptive_field - 1) // 2 # Padding on each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model_run_train.parameters(), lr=lr, amsgrad=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# Initialize loss\n",
    "losses_3d_train = []\n",
    "losses_3d_train_eval = []\n",
    "losses_3d_valid = []\n",
    "\n",
    "# Using batch norm momentum\n",
    "initial_momentum = 0.1\n",
    "final_momentum = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Testing on 2414 frames\n",
      "INFO: Training on 6426 frames\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "valid_generator = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_val, poses_2d=poses_2d_val,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "print('INFO: Testing on {} frames'.format(valid_generator.num_frames()))\n",
    "train_generator = ChunkedGenerator(\n",
    "    batch_size, cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, chunk_length=chunk_length, shuffle=True, augment=False, \n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "train_generator_eval = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, augment=False\n",
    ")\n",
    "print('INFO: Training on {} frames'.format(train_generator_eval.num_frames()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, np.nan])\n",
    "torch.isnan(x).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] time 7.09 lr 0.001 \n",
      "        3d_train 8543141.317377646 \n",
      "        3d_eval 13137641.6015625 \n",
      "        3d_valid 13157423.828125\n",
      "[2] time 6.78 lr 0.00095 \n",
      "        3d_train 8480283.13169886 \n",
      "        3d_eval 13031788.0859375 \n",
      "        3d_valid 13051124.0234375\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "epoch = 0\n",
    "while epoch < num_epochs:\n",
    "    start_time = time.time()\n",
    "    # Initialize training loss\n",
    "    epoch_loss_3d_train = 0\n",
    "    epoch_loss_2d_train_unlabeled = 0\n",
    "    N = 0\n",
    "    # Regular supervised scenario\n",
    "    for _, batch_3d, batch_2d in train_generator.next_epoch():\n",
    "        inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "        inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_3d = inputs_3d.cuda()\n",
    "            inputs_2d = inputs_2d.cuda()\n",
    "        # inputs_3d[:, :, 0] = 0\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict 3D poses (forward) using fp16\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predicted_3d_pos = model_run_train(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss_3d_pos).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "    # Total loss over one epoch\n",
    "    losses_3d_train.append(epoch_loss_3d_train / N)\n",
    "    \n",
    "    # End-of-epoch evaluation\n",
    "    with torch.no_grad():\n",
    "        # Load the newly trained network\n",
    "        model_run.load_state_dict(model_run_train.state_dict())\n",
    "        model_run.eval()\n",
    "        # Initialize validation loss\n",
    "        epoch_loss_3d_valid = 0\n",
    "        epoch_loss_2d_valid = 0\n",
    "        N = 0\n",
    "\n",
    "        # Evaluate on validation dataset\n",
    "        for _, batch_3d, batch_2d in valid_generator.next_epoch():\n",
    "            inputs_3d_valid = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d_valid = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d_valid = inputs_3d_valid.cuda()\n",
    "                inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "            #inputs_3d_valid[:, :, 0] = 0\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d_valid)\n",
    "            N += inputs_3d_valid.shape[0]\n",
    "            epoch_loss_3d_valid += inputs_3d_valid.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_valid.append(epoch_loss_3d_valid / N)\n",
    "\n",
    "        # Evaluate on training set, this time in evaluation mode\n",
    "        epoch_loss_3d_train_eval = 0\n",
    "        epoch_loss_2d_train_labeled_eval = 0\n",
    "        N = 0\n",
    "        for _, batch_3d, batch_2d in train_generator_eval.next_epoch():\n",
    "            if batch_2d.shape[1] == 0:\n",
    "                # This can only happen when downsampling the dataset\n",
    "                continue\n",
    "            \n",
    "            inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d = inputs_3d.cuda()\n",
    "                inputs_2d = inputs_2d.cuda()\n",
    "            #inputs_3d[:, :, 0] = 0\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train_eval += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_train_eval.append(epoch_loss_3d_train_eval / N)\n",
    "\n",
    "    # Calculate total training/validation time over one epoch       \n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f'''[{epoch+1}] time {elapsed:.2f} lr {lr} \n",
    "        3d_train {losses_3d_train[-1] * 1000} \n",
    "        3d_eval {losses_3d_train_eval[-1] * 1000} \n",
    "        3d_valid {losses_3d_valid[-1]  *1000}'''\n",
    "    )\n",
    "\n",
    "    # Decay learning rate exponentially\n",
    "    lr *= lr_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    epoch += 1\n",
    "\n",
    "    # Decay BatchNorm momentum\n",
    "    momentum = initial_momentum * np.exp(\n",
    "        -epoch/num_epochs * np.log(initial_momentum/final_momentum)\n",
    "    )\n",
    "    model_run_train.set_bn_momentum(momentum)\n",
    "\n",
    "    # Save training curves after every epoch, as .png images\n",
    "    plt.figure()\n",
    "    epoch_x = np.arange(3, len(losses_3d_train)) + 1\n",
    "    plt.plot(epoch_x, losses_3d_train[3:], '--', color='C0')\n",
    "    plt.plot(epoch_x, losses_3d_train_eval[3:], color='C0')\n",
    "    plt.plot(epoch_x, losses_3d_valid[3:], color='C1')\n",
    "    plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])\n",
    "    plt.ylabel('MPJPE (m)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xlim((3, epoch))\n",
    "    plt.savefig('loss_plots/' + str(epoch) + '_loss_3d.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint to runningpose/_epoch_2.bin\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "chk_path = os.path.join('runningpose', '_epoch_{}.bin'.format(epoch))\n",
    "print('Saving checkpoint to', chk_path)\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'lr': lr,\n",
    "    'random_state': train_generator.random_state(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_run': model_run_train.state_dict(),\n",
    "}, chk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_model.ipynb.\n",
      "Converted 01_loss.ipynb.\n",
      "Converted 02_skeleton.ipynb.\n",
      "Converted 03_mocap_dataset.ipynb.\n",
      "Converted 04_h36m_dataset.ipynb.\n",
      "Converted 05_camera.ipynb.\n",
      "Converted 06_quaternion.ipynb.\n",
      "Converted 07_utils.ipynb.\n",
      "Converted 08_generators.ipynb.\n",
      "Converted 09_custom_dataset.ipynb.\n",
      "Converted 10_visualization.ipynb.\n",
      "Converted 11_arguments.ipynb.\n",
      "Converted 12_data_utils.ipynb.\n",
      "Converted 13_prepare_data_2d_custom.ipynb.\n",
      "Converted 14_infer_video.ipynb.\n",
      "Converted 15_prepare_data_COCO.ipynb.\n",
      "Converted 16_pycococreatortools.ipynb.\n",
      "Converted 17_format_qtmdata.ipynb.\n",
      "Converted 18_runningpose_dataset.ipynb.\n",
      "Converted 19_train_detectron2.ipynb.\n",
      "Converted 20_transfer_model.ipynb.\n",
      "Converted 21_prepare_data_3d.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
