{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.transfer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning temporal model \n",
    "> Trains the temporal model from the Video Pose3D checkpoint made for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from runningpose.core.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from runningpose.core.loss import mpjpe\n",
    "from runningpose.core.model import TemporalModel\n",
    "from runningpose.core.camera import normalize_screen_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "dict_keys(['Ioanna1_Camera1_170Hz_3D_keypoints', 'Ioanna2_Camera1_170Hz_3D_keypoints', 'Ioanna3_Camera1_170Hz_3D_keypoints', 'Josef1_Camera1_170Hz_3D_keypoints', 'Josef2_Camera1_170Hz_3D_keypoints', 'Josef3_Camera1_170Hz_3D_keypoints', 'Josef4_Camera1_170Hz_3D_keypoints', 'Ioanna1_Camera2_170Hz_3D_keypoints', 'Ioanna2_Camera2_170Hz_3D_keypoints', 'Ioanna3_Camera2_170Hz_3D_keypoints', 'Josef1_Camera2_170Hz_3D_keypoints', 'Josef2_Camera2_170Hz_3D_keypoints', 'Josef3_Camera2_170Hz_3D_keypoints', 'Josef4_Camera2_170Hz_3D_keypoints', 'Ioanna1_Camera3_170Hz_3D_keypoints', 'Ioanna2_Camera3_170Hz_3D_keypoints', 'Ioanna3_Camera3_170Hz_3D_keypoints', 'Josef1_Camera3_170Hz_3D_keypoints', 'Josef2_Camera3_170Hz_3D_keypoints', 'Josef3_Camera3_170Hz_3D_keypoints', 'Josef4_Camera3_170Hz_3D_keypoints'])\n",
      "(374, 18, 3)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading training dataset...')\n",
    "keypoints_3D = np.load('data_files/data_3d_train.npz', allow_pickle=True)\n",
    "keypoints_3D = keypoints_3D['positions_3d'].item()\n",
    "joints_left = [3, 6, 7, 10, 12, 14, 16]\n",
    "joints_right = [4, 8, 9, 11, 13, 15, 17]\n",
    "print(keypoints_3D.keys())\n",
    "print(keypoints_3D['Ioanna3_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a dictionary with all the videos and then a \n",
    "custom dictionary with a list with an array with (frames, keypoints, dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D training detections...\n",
      "dict_keys(['miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', 'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', 'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', 'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', 'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi', 'miqus3_Ioanna_01.avi', 'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_01.avi', 'miqus3_Josef_02.avi', 'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'])\n",
      "(323, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D = np.load('data_files/data_2d_custom_trainingdata.npz', allow_pickle=True)\n",
    "keypoints_2D_metadata = keypoints_2D['metadata'].item()\n",
    "keypoints_2D_symmetry = keypoints_2D_metadata['keypoints_symmetry']\n",
    "kps_left = list(keypoints_2D_symmetry[0])\n",
    "kps_right = list(keypoints_2D_symmetry[1])\n",
    "keypoints_2D = keypoints_2D['positions_2d'].item()\n",
    "keypoints_2D = dict(sorted(keypoints_2D.items()))\n",
    "print(keypoints_2D.keys())\n",
    "print(keypoints_2D['miqus1_Ioanna_01.avi']['custom'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize training input data\n",
    "The resolution of the third camera is diffrent due to being cropped. \n",
    "During our datacollection we got some other people in the shot. Why we had to\n",
    "crop the video. \n",
    "\n",
    "First convert the 3D data from millimeter -> meter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Converts all training data to meters\n",
    "for subject in keypoints_3D.keys():\n",
    "    keypoints_3D[subject] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Normalize camera frame\n",
    "subjects = [\n",
    "    'miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', \n",
    "    'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', \n",
    "    'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', \n",
    "    'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', \n",
    "    'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi'\n",
    "]\n",
    "# 2D data \n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1920, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_extra_cut = ['miqus3_Ioanna_01.avi', 'miqus3_Josef_01.avi']\n",
    "for subject in subjects_extra_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1350, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_02.avi', \n",
    "    'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "                kps = normalize_screen_coordinates(kps, w=1480, h=1088)\n",
    "                keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "print('Loading validation dataset...')\n",
    "keypoints_3D_val = np.load('data_files/data_3d_val.npz', allow_pickle=True)\n",
    "keypoints_3D_val = keypoints_3D_val['positions_3d'].item()\n",
    "print(keypoints_3D_val.keys())\n",
    "# print(keypoints_3D['Ioanna1_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D_val = np.load('data_files/data_2d_custom_validationdata.npz', allow_pickle=True)\n",
    "keypoints_2D_val = keypoints_2D_val['positions_2d'].item()\n",
    "keypoints_2D_val = dict(sorted(keypoints_2D_val.items()))\n",
    "print(keypoints_2D_val.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize validation input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Converts all 3D validation data to meters\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    keypoints_3D_val[subject] /= 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects = [\n",
    "    'miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi', \n",
    "    'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1920, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi', \n",
    "    'miqus3_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1480, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and fix errors in data\n",
    "Assert that we have the same number of frames in the 2D and 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Wrong cut .avi should have been 4.42 long but is 4.44 \n",
    "keypoints_2D['miqus3_Josef_03.avi']['custom'][0] = keypoints_2D['miqus3_Josef_03.avi']['custom'][0][:374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "shapes_3d = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    shapes_3d.append(keypoints_3D[subject].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix missing right foot values by inferring from a later frame\n",
    "> We had a missing RForefoot2 in the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera3_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna3_Camera2_170Hz_3D_keypoints'][20, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "shapes_2d = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    shapes_2d.append(keypoints_2D[subject]['custom'][0].shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for i in range(len(shapes_2d)):\n",
    "    assert shapes_2d[i][0] == shapes_3d[i][0], f'subject {i}: {shapes_2d[i][0]}, {shapes_3d[i][0]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root relative coordinates\n",
    "> The 3D predictions should be root relative so we need to convert our 3D data. WaistBack is root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for subject in keypoints_3D.keys():\n",
    "    for i in range(keypoints_3D[subject].shape[0]):\n",
    "        keypoints_3D[subject][i, :, :] -= keypoints_3D[subject][i, 5, :]\n",
    "\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    for i in range(keypoints_3D_val[subject].shape[0]):\n",
    "        keypoints_3D_val[subject][i, :, :] -= keypoints_3D_val[subject][i, 5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Roll out all the data to lists for the generators to create batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "poses_2d_train = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    poses_2d_train.append(keypoints_2D[subject]['custom'][0])\n",
    "\n",
    "poses_3d_train = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    poses_3d_train.append(keypoints_3D[subject])\n",
    "\n",
    "assert len(poses_2d_train) == len(poses_3d_train), \"Number of runs doesn't match.\"\n",
    "\n",
    "poses_2d_val = []\n",
    "for subject in keypoints_2D_val.keys():\n",
    "    poses_2d_val.append(keypoints_2D_val[subject]['custom'][0])\n",
    "\n",
    "poses_3d_val = []\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    poses_3d_val.append(keypoints_3D_val[subject])\n",
    "\n",
    "assert len(poses_2d_val) == len(poses_3d_val), \"Number of runs doesn't match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "> Load the temporal model trained model for generating 3D predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('pretrained_h36m_detectron_coco.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then\n",
    "initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on\n",
    "your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up\n",
    "convergence and eliminate “hockey stick” loss curves - Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Check what the output mean is for each keypoint over all subjects.\n",
    "allsub_keypoints = 0\n",
    "for subject in keypoints_3D.keys():\n",
    "    allsub_keypoints += np.mean(keypoints_3D[subject], axis=0)\n",
    "\n",
    "mean_keypoints = allsub_keypoints / len(keypoints_3D.keys())\n",
    "mean_keypoints = np.reshape(\n",
    "    mean_keypoints, (mean_keypoints.shape[0]*mean_keypoints.shape[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "# Hyperparameters\n",
    "num_joints_in = 17 # COCO\n",
    "in_features = 2 # dimension of in joints\n",
    "num_joints_out = 18\n",
    "filter_widths = [3, 3, 3, 3, 3] # just as in inference  \n",
    "causal = False # No real time predictions \n",
    "dropout = 0.125\n",
    "channels = 1024 # default\n",
    "lr = 3e-3\n",
    "lr_decay = 0.97\n",
    "batch_size = 128\n",
    "chunk_length = 1\n",
    "num_epochs = 230\n",
    "unfreeze_epoch = 4\n",
    "trigger_times = 0\n",
    "patience = 6000 #################NOTE: Turned off\n",
    "\n",
    "# Load two models one for training and one for evaluation\n",
    "model_run_train = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "\n",
    "# Reintizialize the last output layer to fit new out. \n",
    "checkpoint['model_pos']['shrink.weight'] = torch.randn(num_joints_out*3, channels, 1)\n",
    "checkpoint['model_pos']['shrink.bias'] = torch.from_numpy(mean_keypoints)\n",
    "\n",
    "# Load the pretrained model i.e to do transfer learning\n",
    "model_run_train.load_state_dict(checkpoint['model_pos'])\n",
    "\n",
    "# Freeze all layers except the last new layer\n",
    "for name, param in model_run_train.named_parameters():\n",
    "    if name != 'shrink.weight' and name != 'shrink.bias':\n",
    "        param.requires_grad = False\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_run_train = model_run_train.cuda()\n",
    "    model_run = model_run.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "# Calculate padding based on receptive field\n",
    "receptive_field = model_run_train.receptive_field()\n",
    "print('INFO: Receptive field: {} frames'.format(receptive_field))\n",
    "pad = (receptive_field - 1) // 2 # Padding on each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model_run_train.parameters(), lr=lr, amsgrad=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# Initialize loss\n",
    "losses_3d_train = []\n",
    "losses_3d_train_eval = []\n",
    "losses_3d_valid = []\n",
    "\n",
    "# Using batch norm momentum\n",
    "initial_momentum = 0.1\n",
    "final_momentum = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "valid_generator = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_val, poses_2d=poses_2d_val,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "print('INFO: Validating on {} frames'.format(valid_generator.num_frames()))\n",
    "\n",
    "train_generator = ChunkedGenerator(\n",
    "    batch_size, cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, chunk_length=chunk_length, shuffle=True, augment=True, \n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "train_generator_eval = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, augment=False\n",
    ")\n",
    "print('INFO: Training on {} frames'.format(train_generator_eval.num_frames()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Trains with freezed layers first then moves on training the whole net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "epoch = 0\n",
    "while epoch < num_epochs:\n",
    "    start_time = time.time()\n",
    "    # Unfreezes the layers after a given epoch\n",
    "    if epoch == unfreeze_epoch:\n",
    "        lr = 3e-4\n",
    "        print('Unfreezes the model after epoch:', epoch)\n",
    "        for param in model_run_train.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Initialize training loss\n",
    "    epoch_loss_3d_train = 0\n",
    "    epoch_loss_2d_train_unlabeled = 0\n",
    "    N = 0\n",
    "    # Regular supervised scenario\n",
    "    for _, batch_3d, batch_2d in train_generator.next_epoch():\n",
    "        inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "        inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_3d = inputs_3d.cuda()\n",
    "            inputs_2d = inputs_2d.cuda()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Predict 3D poses (forward) using fp16\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predicted_3d_pos = model_run_train(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss_3d_pos).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "    # Total loss over one epoch\n",
    "    losses_3d_train.append(epoch_loss_3d_train / N)\n",
    "    \n",
    "    # End-of-epoch evaluation\n",
    "    with torch.no_grad():\n",
    "        # Load the newly trained network\n",
    "        model_run.load_state_dict(model_run_train.state_dict())\n",
    "        model_run.eval()\n",
    "        # Initialize validation loss\n",
    "        epoch_loss_3d_valid = 0\n",
    "        N = 0\n",
    "\n",
    "        # Evaluate on validation dataset\n",
    "        for _, batch_3d, batch_2d in valid_generator.next_epoch():\n",
    "            inputs_3d_valid = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d_valid = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d_valid = inputs_3d_valid.cuda()\n",
    "                inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d_valid)\n",
    "            N += inputs_3d_valid.shape[0]\n",
    "            epoch_loss_3d_valid += inputs_3d_valid.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_valid.append(epoch_loss_3d_valid / N)\n",
    "\n",
    "        # Early stopping\n",
    "        # if epoch > 1:\n",
    "        #     if losses_3d_valid[-1] > losses_3d_valid[-2]:\n",
    "        #         trigger_times += 1\n",
    "        #         print('Trigger Times:', trigger_times, flush=True)\n",
    "\n",
    "        #         if trigger_times > patience:\n",
    "        #             print('Early stopping! at epoch:', epoch+1)\n",
    "        #             epoch = num_epochs\n",
    "                    \n",
    "        # Evaluate on training set, this time in evaluation mode\n",
    "        epoch_loss_3d_train_eval = 0\n",
    "        epoch_loss_2d_train_labeled_eval = 0\n",
    "        N = 0\n",
    "        for _, batch_3d, batch_2d in train_generator_eval.next_epoch():\n",
    "            if batch_2d.shape[1] == 0:\n",
    "                # This can only happen when downsampling the dataset\n",
    "                continue\n",
    "            \n",
    "            inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d = inputs_3d.cuda()\n",
    "                inputs_2d = inputs_2d.cuda()\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train_eval += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_train_eval.append(epoch_loss_3d_train_eval / N)\n",
    "\n",
    "    # Calculate total training/validation time over one epoch       \n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f'''[{epoch+1}] time {elapsed:.2f} lr {lr} \n",
    "        3d_train {losses_3d_train[-1] * 1000} \n",
    "        3d_eval {losses_3d_train_eval[-1] * 1000} \n",
    "        3d_valid {losses_3d_valid[-1]  * 1000}''', \n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    # Decay learning rate exponentially\n",
    "    lr *= lr_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    epoch += 1\n",
    "\n",
    "    # Decay BatchNorm momentum\n",
    "    momentum = initial_momentum * np.exp(\n",
    "        -epoch/num_epochs * np.log(initial_momentum/final_momentum)\n",
    "    )\n",
    "    model_run_train.set_bn_momentum(momentum)\n",
    "\n",
    "    # Save training curves after every epoch, as .png images\n",
    "    if epoch%10 == 0:\n",
    "        plt.figure()\n",
    "        epoch_x = np.arange(8, len(losses_3d_train)) + 1\n",
    "        plt.plot(epoch_x, losses_3d_train[8:], '--', color='C0')\n",
    "        plt.plot(epoch_x, losses_3d_train_eval[8:], color='C0')\n",
    "        plt.plot(epoch_x, losses_3d_valid[8:], color='C1')\n",
    "        plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])\n",
    "        plt.ylabel('MPJPE (m)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((8, epoch))\n",
    "        plt.savefig('loss_plots/' + str(epoch) + '_loss_3d.png')\n",
    "        plt.close('all')\n",
    "    \n",
    "    if epoch >= 50:\n",
    "        chk_path = os.path.join('runningpose_epoch_{}.bin'.format(epoch))\n",
    "        print(chk_path)\n",
    "        print('Saving checkpoint to', chk_path)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'lr': lr,\n",
    "            'random_state': train_generator.random_state(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_run': model_run_train.state_dict(),\n",
    "        }, chk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "chk_path = os.path.join('runningpose_epoch_{}.bin'.format(epoch))\n",
    "print(chk_path)\n",
    "print('Saving checkpoint to', chk_path)\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'lr': lr,\n",
    "    'random_state': train_generator.random_state(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_run': model_run_train.state_dict(),\n",
    "}, chk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualize the predictions on one of our validation videos\n",
    "> This is highly temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "This model was trained for 100 epochs\n",
      "Rendering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.3.2 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 10.3.0 (GCC)\n",
      "  configuration: --prefix=/home/conda/feedstock_root/build_artifacts/ffmpeg_1645955405450/_h_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_plac --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1645955405450/_build_env/bin/x86_64-conda-linux-gnu-cc --disable-doc --disable-openssl --enable-avresample --enable-gnutls --enable-gpl --enable-hardcoded-tables --enable-libfreetype --enable-libopenh264 --enable-libx264 --enable-pic --enable-pthreads --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libmp3lame --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1645955405450/_build_env/bin/pkg-config\n",
      "  libavutil      56. 51.100 / 56. 51.100\n",
      "  libavcodec     58. 91.100 / 58. 91.100\n",
      "  libavformat    58. 45.100 / 58. 45.100\n",
      "  libavdevice    58. 10.100 / 58. 10.100\n",
      "  libavfilter     7. 85.100 /  7. 85.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  7.100 /  5.  7.100\n",
      "  libswresample   3.  7.100 /  3.  7.100\n",
      "  libpostproc    55.  7.100 / 55.  7.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'sample_run.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp42mp41isomavc1\n",
      "    creation_time   : 2020-05-20T09:13:36.000000Z\n",
      "  Duration: 00:00:22.80, start: 0.000000, bitrate: 5734 kb/s\n",
      "    Stream #0:0(und): Video: h264 (High) (avc1 / 0x31637661), yuv420p(tv, bt709), 1920x1080, 5730 kb/s, 50 fps, 50 tbr, 50 tbn, 100 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2020-05-20T09:13:36.000000Z\n",
      "      handler_name    : L-SMASH Video Handler\n",
      "      encoder         : AVC Coding\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (h264 (native) -> rawvideo (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, image2pipe, to 'pipe:':\n",
      "  Metadata:\n",
      "    major_brand     : mp42\n",
      "    minor_version   : 0\n",
      "    compatible_brands: mp42mp41isomavc1\n",
      "    encoder         : Lavf58.45.100\n",
      "    Stream #0:0(und): Video: rawvideo (RGB[24] / 0x18424752), rgb24, 1920x1080, q=2-31, 2488320 kb/s, 50 fps, 50 tbn, 50 tbc (default)\n",
      "    Metadata:\n",
      "      creation_time   : 2020-05-20T09:13:36.000000Z\n",
      "      handler_name    : L-SMASH Video Handler\n",
      "      encoder         : Lavc58.91.100 rawvideo\n",
      "frame= 1140 fps=252 q=-0.0 Lsize= 6925500kB time=00:00:22.80 bitrate=2488320.0kbits/s speed=5.04x    \n",
      "video:6925500kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.000000%\n",
      "/media/richard/SecondLinux/repos/runningpose/nbs/runningpose/core/visualization.py:279: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all axes decorations.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1139/1140\r"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from runningpose.core.skeleton import Skeleton\n",
    "runningpose_skeleton = Skeleton(\n",
    "    parents=[-1, 0, 1, 1, 1, 2, 5, 16, 5, 17, 3, 4, 10, 11, 6, 8, 14, 15],\n",
    "    joints_left=[3, 6, 7, 10, 12, 14, 16], \n",
    "    joints_right=[4, 8, 9, 11, 13, 15, 17]\n",
    ")\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('runningpose_100.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model_run = model_run.cuda()\n",
    "\n",
    "model_run.load_state_dict(checkpoint['model_run'])\n",
    "\n",
    "from runningpose.core.runningpose_dataset import runningpose_cameras_extrinsic_params\n",
    "from runningpose.core.camera import camera_to_world_miqus, image_coordinates\n",
    "from runningpose.core.visualization import render_animation\n",
    "tindra_cam = keypoints_2D_val['miqus3_Tindra_03.avi']['custom']\n",
    "josef_cam1 = keypoints_2D['miqus1_Josef_01.avi']['custom']\n",
    "sample_run = keypoints_2D_test['sample_run.mp4']['custom']\n",
    "gen = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=None, poses_2d=sample_run,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "predicted = 0\n",
    "data_world = 0\n",
    "print('Rendering...')\n",
    "with torch.no_grad():\n",
    "    # Load the newly trained network\n",
    "    model_run.eval()\n",
    "\n",
    "    for _, _, batch2d in gen.next_epoch():\n",
    "        inputs_2d_valid = torch.from_numpy(batch2d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "\n",
    "        # Predict 3D poses (forward)\n",
    "        predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "        # Convert predicted 3d poses to world coordinates. \n",
    "        predicted_3d_pos = predicted_3d_pos.cpu().detach().numpy()[0]\n",
    "        predicted = predicted_3d_pos\n",
    "        predicted_3d_pos = predicted_3d_pos.transpose(1, 0, 2)\n",
    "    \n",
    "        # Get camera parameters.\n",
    "        R = runningpose_cameras_extrinsic_params[0]['rotation']\n",
    "        T = np.array([runningpose_cameras_extrinsic_params[0]['translation']]).T\n",
    "        data_3D_world = []\n",
    "        for keypoint in predicted_3d_pos:\n",
    "            data_3D_world.append(camera_to_world_miqus(keypoint, R, T=0))\n",
    "\n",
    "        data_3D_world = np.array(data_3D_world).transpose(1, 0, 2)\n",
    "        data_3D_world[:, :, 2] -= np.min(data_3D_world[:, :, 2])\n",
    "        data_world = data_3D_world\n",
    "        \n",
    "        anim_output = {'Reconstruction': data_3D_world}\n",
    "        input_keypoints = image_coordinates(sample_run[0][..., :2], w=1920, h=1080)\n",
    "        \n",
    "        render_animation(\n",
    "            input_keypoints, keypoints_2D_metadata, anim_output,\n",
    "            runningpose_skeleton, 85, 3000, 70, 'sample_run.gif', size=12, \n",
    "            input_video_path='sample_run.mp4', viewport=(1920, 1080)\n",
    "        )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the trained models on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "keypoints_3D_test = np.load('data_files/data_3d_test.npz', allow_pickle=True)\n",
    "keypoints_3D_test = keypoints_3D_test['positions_3d'].item()\n",
    "keypoints_3D_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D_test = np.load('data_2d_custom_testdata.npz', allow_pickle=True)\n",
    "keypoints_2D_metadata = keypoints_2D_test['metadata'].item()\n",
    "keypoints_2D_symmetry = keypoints_2D_metadata['keypoints_symmetry']\n",
    "kps_left = list(keypoints_2D_symmetry[0])\n",
    "kps_right = list(keypoints_2D_symmetry[1])\n",
    "keypoints_2D_test = keypoints_2D_test['positions_2d'].item()\n",
    "keypoints_2D_test = dict(sorted(keypoints_2D_test.items()))\n",
    "print(keypoints_2D_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "keypoints_3D_test['Anna2_Camera1_170Hz_3D_keypoints'] = keypoints_3D_test['Anna2_Camera1_170Hz_3D_keypoints'][:, 1:]\n",
    "keypoints_3D_test['Anna2_Camera2_170Hz_3D_keypoints'] = keypoints_3D_test['Anna2_Camera2_170Hz_3D_keypoints'][:, 1:]\n",
    "keypoints_3D_test['Anna2_Camera3_170Hz_3D_keypoints'] = keypoints_3D_test['Anna2_Camera3_170Hz_3D_keypoints'][:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Scale the data \n",
    "for subject in keypoints_3D_test.keys():\n",
    "    keypoints_3D_test[subject] /= 1000\n",
    "\n",
    "for subject in keypoints_3D_test.keys():\n",
    "    for i in range(keypoints_3D_test[subject].shape[0]):\n",
    "        keypoints_3D_test[subject][i, :, :] -= keypoints_3D_test[subject][i, 5, :]\n",
    "\n",
    "subjects = ['Anna_Miqus_14.avi', 'Anna_Miqus_15.avi']\n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D_test[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_test[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1920, h=1088)\n",
    "            keypoints_2D_test[subject][action][idx] = kps\n",
    "\n",
    "subjects = ['Anna_Miqus_16.avi']\n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D_test[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_test[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1480, h=1088)\n",
    "            keypoints_2D_test[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "poses_2d_test = []\n",
    "for subject in keypoints_2D_test.keys():\n",
    "    poses_2d_test.append(keypoints_2D_test[subject]['custom'][0])\n",
    "\n",
    "poses_3d_test = []\n",
    "for subject in keypoints_3D_test.keys():\n",
    "    poses_3d_test.append(keypoints_3D_test[subject])\n",
    "\n",
    "assert len(poses_2d_test) == len(poses_3d_test), \"Number of runs doesn't match.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('runningpose_100.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "if torch.cuda.is_available():\n",
    "    model_run = model_run.cuda()\n",
    "\n",
    "model_run.load_state_dict(checkpoint['model_run'])\n",
    "\n",
    "testing_generator = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_test, poses_2d=poses_2d_test,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "print('INFO: Testing on {} frames'.format(testing_generator.num_frames()))\n",
    "\n",
    "losses_3d_test = []\n",
    "print('Predicting...')\n",
    "with torch.no_grad():\n",
    "    # Load the newly trained network\n",
    "    model_run.eval()\n",
    "    \n",
    "    # Initialize validation loss\n",
    "    epoch_loss_3d_test = 0\n",
    "    N = 0\n",
    "    for _, batch_3d, batch2d in testing_generator.next_epoch():\n",
    "        inputs_2d_test = torch.from_numpy(batch2d.astype('float32'))\n",
    "        inputs_3d_test = torch.from_numpy(batch_3d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_2d_test = inputs_2d_test.cuda()\n",
    "            inputs_3d_test = inputs_3d_test.cuda()\n",
    "\n",
    "        # Predict 3D poses (forward)\n",
    "        predicted_3d_pos = model_run(inputs_2d_test)\n",
    "        loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d_test)\n",
    "        N += inputs_3d_test.shape[0]\n",
    "        epoch_loss_3d_test += inputs_3d_test.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "    # Total loss over one epoch\n",
    "    losses_3d_test.append(epoch_loss_3d_test / N)\n",
    "\n",
    "    print(losses_3d_test[-1] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('ffmpeg')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
