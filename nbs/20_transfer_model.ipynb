{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.transfer_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning temporal model \n",
    "> Trains the temporal model from the Video Pose3D checkpoint made for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from runningpose.core.generators import ChunkedGenerator, UnchunkedGenerator\n",
    "from runningpose.core.loss import mpjpe\n",
    "from runningpose.core.model import TemporalModel\n",
    "from runningpose.core.camera import normalize_screen_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "#### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "dict_keys(['Ioanna1_Camera1_170Hz_3D_keypoints', 'Ioanna2_Camera1_170Hz_3D_keypoints', 'Ioanna3_Camera1_170Hz_3D_keypoints', 'Josef1_Camera1_170Hz_3D_keypoints', 'Josef2_Camera1_170Hz_3D_keypoints', 'Josef3_Camera1_170Hz_3D_keypoints', 'Josef4_Camera1_170Hz_3D_keypoints', 'Ioanna1_Camera2_170Hz_3D_keypoints', 'Ioanna2_Camera2_170Hz_3D_keypoints', 'Ioanna3_Camera2_170Hz_3D_keypoints', 'Josef1_Camera2_170Hz_3D_keypoints', 'Josef2_Camera2_170Hz_3D_keypoints', 'Josef3_Camera2_170Hz_3D_keypoints', 'Josef4_Camera2_170Hz_3D_keypoints', 'Ioanna1_Camera3_170Hz_3D_keypoints', 'Ioanna2_Camera3_170Hz_3D_keypoints', 'Ioanna3_Camera3_170Hz_3D_keypoints', 'Josef1_Camera3_170Hz_3D_keypoints', 'Josef2_Camera3_170Hz_3D_keypoints', 'Josef3_Camera3_170Hz_3D_keypoints', 'Josef4_Camera3_170Hz_3D_keypoints'])\n",
      "(374, 18, 3)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading training dataset...')\n",
    "keypoints_3D = np.load('data_3d_train.npz', allow_pickle=True)\n",
    "keypoints_3D = keypoints_3D['positions_3d'].item()\n",
    "joints_left = [3, 6, 7, 10, 12, 14, 16]\n",
    "joints_right = [4, 8, 9, 11, 13, 15, 17]\n",
    "print(keypoints_3D.keys())\n",
    "print(keypoints_3D['Ioanna3_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a dictionary with all the videos and then a \n",
    "custom dictionary with a list with an array with (frames, keypoints, dim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D training detections...\n",
      "dict_keys(['miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', 'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', 'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', 'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', 'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi', 'miqus3_Ioanna_01.avi', 'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_01.avi', 'miqus3_Josef_02.avi', 'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'])\n",
      "(323, 17, 2)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D = np.load('data_2d_custom_trainingdata.npz', allow_pickle=True)\n",
    "keypoints_2D_metadata = keypoints_2D['metadata'].item()\n",
    "keypoints_2D_symmetry = keypoints_2D_metadata['keypoints_symmetry']\n",
    "kps_left = list(keypoints_2D_symmetry[0])\n",
    "kps_right = list(keypoints_2D_symmetry[1])\n",
    "keypoints_2D = keypoints_2D['positions_2d'].item()\n",
    "keypoints_2D = dict(sorted(keypoints_2D.items()))\n",
    "print(keypoints_2D.keys())\n",
    "print(keypoints_2D['miqus1_Ioanna_01.avi']['custom'][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize training inputs\n",
    "The resolution of the third camera is diffrent due to being cropped. \n",
    "During our datacollection we got some other people in the shot. Why we had to\n",
    "crop the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Normalize camera frame\n",
    "subjects = [\n",
    "    'miqus1_Ioanna_01.avi', 'miqus1_Ioanna_02.avi', 'miqus1_Ioanna_03.avi', \n",
    "    'miqus1_Josef_01.avi', 'miqus1_Josef_02.avi', 'miqus1_Josef_03.avi', \n",
    "    'miqus1_Josef_04.avi', 'miqus2_Ioanna_01.avi', 'miqus2_Ioanna_02.avi', \n",
    "    'miqus2_Ioanna_03.avi', 'miqus2_Josef_01.avi', 'miqus2_Josef_02.avi', \n",
    "    'miqus2_Josef_03.avi', 'miqus2_Josef_04.avi'\n",
    "]\n",
    "# 2D data \n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1920, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_extra_cut = ['miqus3_Ioanna_01.avi', 'miqus3_Josef_01.avi']\n",
    "for subject in subjects_extra_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "            kps = normalize_screen_coordinates(kps, w=1350, h=1088)\n",
    "            keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Ioanna_02.avi', 'miqus3_Ioanna_03.avi', 'miqus3_Josef_02.avi', \n",
    "    'miqus3_Josef_03.avi', 'miqus3_Josef_04.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D[subject][action]):\n",
    "                kps = normalize_screen_coordinates(kps, w=1480, h=1088)\n",
    "                keypoints_2D[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation dataset...\n",
      "dict_keys(['Tindra1_Camera1_170Hz_3D_keypoints', 'Tindra2_Camera1_170Hz_3D_keypoints', 'Tindra3_Camera1_170Hz_3D_keypoints', 'Tindra1_Camera2_170Hz_3D_keypoints', 'Tindra2_Camera2_170Hz_3D_keypoints', 'Tindra3_Camera2_170Hz_3D_keypoints', 'Tindra1_Camera3_170Hz_3D_keypoints', 'Tindra2_Camera3_170Hz_3D_keypoints', 'Tindra3_Camera3_170Hz_3D_keypoints'])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading validation dataset...')\n",
    "keypoints_3D_val = np.load('data_3d_val.npz', allow_pickle=True)\n",
    "keypoints_3D_val = keypoints_3D_val['positions_3d'].item()\n",
    "print(keypoints_3D_val.keys())\n",
    "# print(keypoints_3D['Ioanna1_Camera1_170Hz_3D_keypoints'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2D training detections...\n",
      "dict_keys(['miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi', 'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi', 'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi', 'miqus3_Tindra_03.avi'])\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "print('Loading 2D training detections...')\n",
    "keypoints_2D_val = np.load('data_2d_custom_validationdata.npz', allow_pickle=True)\n",
    "keypoints_2D_val = keypoints_2D_val['positions_2d'].item()\n",
    "keypoints_2D_val = dict(sorted(keypoints_2D_val.items()))\n",
    "print(keypoints_2D_val.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize validation input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects = [\n",
    "    'miqus1_Tindra_01.avi', 'miqus1_Tindra_02.avi', 'miqus1_Tindra_03.avi', \n",
    "    'miqus2_Tindra_01.avi', 'miqus2_Tindra_02.avi', 'miqus2_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1920, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "subjects_cut = [\n",
    "    'miqus3_Tindra_01.avi', 'miqus3_Tindra_02.avi', \n",
    "    'miqus3_Tindra_03.avi'\n",
    "]\n",
    "for subject in subjects_cut:\n",
    "    for action in keypoints_2D_val[subject]:\n",
    "        for idx, kps in enumerate(keypoints_2D_val[subject][action]):\n",
    "            kps[..., :2] = normalize_screen_coordinates(kps[..., :2], w=1480, h=1088)\n",
    "            keypoints_2D_val[subject][action][idx] = kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check and fix errors in data\n",
    "Assert that we have the same number of frames in the 2D and 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Wrong cut .avi should have been 4.42 long but is 4.44 \n",
    "keypoints_2D['miqus3_Josef_03.avi']['custom'][0] = keypoints_2D['miqus3_Josef_03.avi']['custom'][0][:374]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "shapes_3d = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    shapes_3d.append(keypoints_3D[subject].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix missing right foot values by inferring from a later frame\n",
    "> We had a missing RForefoot2 in the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera1_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna2_Camera2_170Hz_3D_keypoints'][20, 9]\n",
    "keypoints_3D['Ioanna2_Camera3_170Hz_3D_keypoints'][:19, 9] = keypoints_3D['Ioanna3_Camera2_170Hz_3D_keypoints'][20, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "shapes_2d = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    shapes_2d.append(keypoints_2D[subject]['custom'][0].shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "for i in range(len(shapes_2d)):\n",
    "    assert shapes_2d[i][0] == shapes_3d[i][0], f'subject {i}: {shapes_2d[i][0]}, {shapes_3d[i][0]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Root relative coordinates\n",
    "> The 3D predictions should be root relative so we need to convert our 3D data. WaistBack is root. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in keypoints_3D.keys():\n",
    "    for i in range(keypoints_3D[subject].shape[0]):\n",
    "        keypoints_3D[subject][i, :, :] -= keypoints_3D[subject][i, 5, :]\n",
    "\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    for i in range(keypoints_3D_val[subject].shape[0]):\n",
    "        keypoints_3D_val[subject][i, :, :] -= keypoints_3D_val[subject][i, 5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roll out all the data to lists for the generators to create batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "poses_2d_train = []\n",
    "for subject in keypoints_2D.keys():\n",
    "    poses_2d_train.append(keypoints_2D[subject]['custom'][0])\n",
    "\n",
    "poses_3d_train = []\n",
    "for subject in keypoints_3D.keys():\n",
    "    poses_3d_train.append(keypoints_3D[subject])\n",
    "\n",
    "assert len(poses_2d_train) == len(poses_3d_train), \"Number of runs doesn't match.\"\n",
    "\n",
    "poses_2d_val = []\n",
    "for subject in keypoints_2D_val.keys():\n",
    "    poses_2d_val.append(keypoints_2D_val[subject]['custom'][0])\n",
    "\n",
    "poses_3d_val = []\n",
    "for subject in keypoints_3D_val.keys():\n",
    "    poses_3d_val.append(keypoints_3D_val[subject])\n",
    "\n",
    "assert len(poses_2d_val) == len(poses_3d_val), \"Number of runs doesn't match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the model\n",
    "> Load the temporal model trained model for generating 3D predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint\n",
      "This model was trained for 80 epochs\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Load checkpoint\n",
    "print('Loading checkpoint')\n",
    "checkpoint = torch.load('pretrained_h36m_detectron_coco.bin', \n",
    "                        map_location=lambda storage,\n",
    "                        loc: storage)\n",
    "print('This model was trained for {} epochs'.format(checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize the final layer weights correctly. E.g. if you are regressing some values that have a mean of 50 then\n",
    "initialize the final bias to 50. If you have an imbalanced dataset of a ratio 1:10 of positives:negatives, set the bias on\n",
    "your logits such that your network predicts probability of 0.1 at initialization. Setting these correctly will speed up\n",
    "convergence and eliminate “hockey stick” loss curves - Andrej Karpathy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what the output mean is for each keypoint over all subjects.\n",
    "allsub_keypoints = 0\n",
    "for subject in keypoints_3D.keys():\n",
    "    allsub_keypoints += np.mean(keypoints_3D[subject], axis=0)\n",
    "\n",
    "mean_keypoints = allsub_keypoints / len(keypoints_3D.keys())\n",
    "mean_keypoints = np.reshape(\n",
    "    mean_keypoints, (mean_keypoints.shape[0]*mean_keypoints.shape[1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#export\n",
    "# Hyperparameters\n",
    "num_joints_in = 17 # COCO\n",
    "in_features = 2 # dimension of in joints\n",
    "num_joints_out = 18\n",
    "filter_widths = [3, 3, 3, 3, 3] # just as in inference  \n",
    "causal = False # No real time predictions \n",
    "dropout = 0.333\n",
    "channels = 1024 # default\n",
    "lr = 3e-4\n",
    "lr_decay = 1\n",
    "batch_size = 256\n",
    "chunk_length = 1\n",
    "num_epochs = 50\n",
    "trigger_times = 0\n",
    "patience = 4\n",
    "\n",
    "# Load two models one for training and one for evaluation\n",
    "model_run_train = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "model_run = TemporalModel(\n",
    "    num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "    dropout, channels\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model_run_train = model_run_train.cuda()\n",
    "    model_run = model_run.cuda()\n",
    "\n",
    "# Reintizialize the last output layer to fit new out. \n",
    "checkpoint['model_pos']['shrink.weight'] = torch.randn(num_joints_out*3, channels, 1)\n",
    "checkpoint['model_pos']['shrink.bias'] = torch.from_numpy(mean_keypoints)\n",
    "\n",
    "# Load the pretrained model i.e to do transfer learning\n",
    "model_run_train.load_state_dict(checkpoint['model_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Receptive field: 243 frames\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "# Calculate padding based on receptive field\n",
    "receptive_field = model_run_train.receptive_field()\n",
    "print('INFO: Receptive field: {} frames'.format(receptive_field))\n",
    "pad = (receptive_field - 1) // 2 # Padding on each side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model_run_train.parameters(), lr=lr, amsgrad=True)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "# Initialize loss\n",
    "losses_3d_train = []\n",
    "losses_3d_train_eval = []\n",
    "losses_3d_valid = []\n",
    "\n",
    "# Using batch norm momentum\n",
    "initial_momentum = 0.1\n",
    "final_momentum = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Testing on 2414 frames\n",
      "INFO: Training on 6426 frames\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "valid_generator = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_val, poses_2d=poses_2d_val,\n",
    "    pad=pad, augment=False,\n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "print('INFO: Testing on {} frames'.format(valid_generator.num_frames()))\n",
    "\n",
    "train_generator = ChunkedGenerator(\n",
    "    batch_size, cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, chunk_length=chunk_length, shuffle=True, augment=True, \n",
    "    kps_left=kps_left, kps_right=kps_right, \n",
    "    joints_left=joints_left, joints_right=joints_right\n",
    ")\n",
    "train_generator_eval = UnchunkedGenerator(\n",
    "    cameras=None, poses_3d=poses_3d_train, poses_2d=poses_2d_train, \n",
    "    pad=pad, augment=False\n",
    ")\n",
    "print('INFO: Training on {} frames'.format(train_generator_eval.num_frames()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] time 14.27 lr 0.0003 \n",
      "        3d_train 314138.6646848952 \n",
      "        3d_eval 303476.1962890625 \n",
      "        3d_valid 308846.00830078125\n",
      "[2] time 13.92 lr 0.0003 \n",
      "        3d_train 249054.11087639266 \n",
      "        3d_eval 258733.67309570312 \n",
      "        3d_valid 268765.56396484375\n",
      "[3] time 13.99 lr 0.0003 \n",
      "        3d_train 214062.5477377366 \n",
      "        3d_eval 222258.6212158203 \n",
      "        3d_valid 240652.13012695312\n",
      "[4] time 13.96 lr 0.0003 \n",
      "        3d_train 186677.80740632841 \n",
      "        3d_eval 191466.76635742188 \n",
      "        3d_valid 213210.0830078125\n",
      "[5] time 14.00 lr 0.0003 \n",
      "        3d_train 163529.8945560212 \n",
      "        3d_eval 168672.08862304688 \n",
      "        3d_valid 198295.10498046875\n",
      "[6] time 14.07 lr 0.0003 \n",
      "        3d_train 143934.87667555085 \n",
      "        3d_eval 143263.1072998047 \n",
      "        3d_valid 178442.24548339844\n",
      "[7] time 14.11 lr 0.0003 \n",
      "        3d_train 125641.75582206625 \n",
      "        3d_eval 115643.4326171875 \n",
      "        3d_valid 154994.27795410156\n",
      "[8] time 14.16 lr 0.0003 \n",
      "        3d_train 109343.67791611273 \n",
      "        3d_eval 97781.7611694336 \n",
      "        3d_valid 142876.7547607422\n",
      "[9] time 14.08 lr 0.0003 \n",
      "        3d_train 95588.9335891149 \n",
      "        3d_eval 76492.85888671875 \n",
      "        3d_valid 128048.84338378906\n",
      "[10] time 14.04 lr 0.0003 \n",
      "        3d_train 83999.73713138413 \n",
      "        3d_eval 74615.20385742188 \n",
      "        3d_valid 126935.05859375\n",
      "[11] time 14.09 lr 0.0003 \n",
      "        3d_train 74984.65570615453 \n",
      "        3d_eval 59358.612060546875 \n",
      "        3d_valid 112903.25927734375\n",
      "[12] time 14.13 lr 0.0003 \n",
      "        3d_train 68150.53783218474 \n",
      "        3d_eval 54509.620666503906 \n",
      "        3d_valid 112533.60748291016\n",
      "[13] time 14.15 lr 0.0003 \n",
      "        3d_train 63391.270741695116 \n",
      "        3d_eval 45392.47131347656 \n",
      "        3d_valid 105670.24230957031\n",
      "Trigger Times: 1\n",
      "[14] time 14.17 lr 0.0003 \n",
      "        3d_train 60307.81759148701 \n",
      "        3d_eval 46936.3899230957 \n",
      "        3d_valid 113844.36798095703\n",
      "[15] time 14.18 lr 0.0003 \n",
      "        3d_train 58081.66120655619 \n",
      "        3d_eval 36235.9619140625 \n",
      "        3d_valid 104511.34490966797\n",
      "[16] time 14.02 lr 0.0003 \n",
      "        3d_train 56515.17341131738 \n",
      "        3d_eval 33644.405364990234 \n",
      "        3d_valid 100154.70886230469\n",
      "Trigger Times: 2\n",
      "[17] time 14.20 lr 0.0003 \n",
      "        3d_train 54830.74132193296 \n",
      "        3d_eval 37735.923767089844 \n",
      "        3d_valid 109836.47918701172\n",
      "[18] time 14.16 lr 0.0003 \n",
      "        3d_train 54026.14734672216 \n",
      "        3d_eval 30038.24234008789 \n",
      "        3d_valid 103252.79235839844\n",
      "[19] time 14.47 lr 0.0003 \n",
      "        3d_train 52674.64048811256 \n",
      "        3d_eval 26799.53384399414 \n",
      "        3d_valid 102265.31219482422\n",
      "[20] time 14.36 lr 0.0003 \n",
      "        3d_train 51842.0220857983 \n",
      "        3d_eval 27067.779541015625 \n",
      "        3d_valid 102017.48657226562\n",
      "Trigger Times: 3\n",
      "[21] time 14.24 lr 0.0003 \n",
      "        3d_train 51786.06960820217 \n",
      "        3d_eval 29500.35285949707 \n",
      "        3d_valid 104948.44055175781\n",
      "Trigger Times: 4\n",
      "[22] time 14.27 lr 0.0003 \n",
      "        3d_train 50555.38203847619 \n",
      "        3d_eval 29393.627166748047 \n",
      "        3d_valid 106564.14031982422\n",
      "[23] time 14.21 lr 0.0003 \n",
      "        3d_train 49789.659370116045 \n",
      "        3d_eval 26626.415252685547 \n",
      "        3d_valid 106550.67443847656\n",
      "[24] time 14.21 lr 0.0003 \n",
      "        3d_train 49819.20483206575 \n",
      "        3d_eval 23261.451721191406 \n",
      "        3d_valid 103485.13793945312\n",
      "Trigger Times: 5\n",
      "Early stopping! at epoch: 25\n",
      "[51] time 14.20 lr 0.0003 \n",
      "        3d_train 49171.70293613865 \n",
      "        3d_eval 26884.281158447266 \n",
      "        3d_valid 104100.43334960938\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "epoch = 0\n",
    "while epoch < num_epochs:\n",
    "    start_time = time.time()\n",
    "    # Initialize training loss\n",
    "    epoch_loss_3d_train = 0\n",
    "    epoch_loss_2d_train_unlabeled = 0\n",
    "    N = 0\n",
    "    # Regular supervised scenario\n",
    "    for _, batch_3d, batch_2d in train_generator.next_epoch():\n",
    "        inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "        inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_3d = inputs_3d.cuda()\n",
    "            inputs_2d = inputs_2d.cuda()\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Predict 3D poses (forward) using fp16\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predicted_3d_pos = model_run_train(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Backward\n",
    "        scaler.scale(loss_3d_pos).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    \n",
    "\n",
    "    # Total loss over one epoch\n",
    "    losses_3d_train.append(epoch_loss_3d_train / N)\n",
    "    \n",
    "    # End-of-epoch evaluation\n",
    "    with torch.no_grad():\n",
    "        # Load the newly trained network\n",
    "        model_run.load_state_dict(model_run_train.state_dict())\n",
    "        model_run.eval()\n",
    "        # Initialize validation loss\n",
    "        epoch_loss_3d_valid = 0\n",
    "        epoch_loss_2d_valid = 0\n",
    "        N = 0\n",
    "\n",
    "        # Evaluate on validation dataset\n",
    "        for _, batch_3d, batch_2d in valid_generator.next_epoch():\n",
    "            inputs_3d_valid = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d_valid = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d_valid = inputs_3d_valid.cuda()\n",
    "                inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d_valid)\n",
    "            N += inputs_3d_valid.shape[0]\n",
    "            epoch_loss_3d_valid += inputs_3d_valid.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_valid.append(epoch_loss_3d_valid / N)\n",
    "\n",
    "        # Early stopping\n",
    "        if epoch > 1:\n",
    "            if losses_3d_valid[-1] > losses_3d_valid[-2]:\n",
    "                trigger_times += 1\n",
    "                print('Trigger Times:', trigger_times, flush=True)\n",
    "\n",
    "                if trigger_times > patience:\n",
    "                    print('Early stopping! at epoch:', epoch+1)\n",
    "                    epoch = num_epochs\n",
    "                    \n",
    "        # Evaluate on training set, this time in evaluation mode\n",
    "        epoch_loss_3d_train_eval = 0\n",
    "        epoch_loss_2d_train_labeled_eval = 0\n",
    "        N = 0\n",
    "        for _, batch_3d, batch_2d in train_generator_eval.next_epoch():\n",
    "            if batch_2d.shape[1] == 0:\n",
    "                # This can only happen when downsampling the dataset\n",
    "                continue\n",
    "            \n",
    "            inputs_3d = torch.from_numpy(batch_3d.astype('float32'))\n",
    "            inputs_2d = torch.from_numpy(batch_2d.astype('float32'))\n",
    "            if torch.cuda.is_available():\n",
    "                inputs_3d = inputs_3d.cuda()\n",
    "                inputs_2d = inputs_2d.cuda()\n",
    "\n",
    "            # Predict 3D poses (forward)\n",
    "            predicted_3d_pos = model_run(inputs_2d)\n",
    "            loss_3d_pos = mpjpe(predicted_3d_pos, inputs_3d)\n",
    "            N += inputs_3d.shape[0]\n",
    "            epoch_loss_3d_train_eval += inputs_3d.shape[0] * loss_3d_pos.item()\n",
    "\n",
    "        # Total loss over one epoch\n",
    "        losses_3d_train_eval.append(epoch_loss_3d_train_eval / N)\n",
    "\n",
    "    # Calculate total training/validation time over one epoch       \n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(\n",
    "        f'''[{epoch+1}] time {elapsed:.2f} lr {lr} \n",
    "        3d_train {losses_3d_train[-1] * 1000} \n",
    "        3d_eval {losses_3d_train_eval[-1] * 1000} \n",
    "        3d_valid {losses_3d_valid[-1]  *1000}''', \n",
    "        flush=True\n",
    "    )\n",
    "\n",
    "    # Decay learning rate exponentially\n",
    "    lr *= lr_decay\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= lr_decay\n",
    "    epoch += 1\n",
    "\n",
    "    # Decay BatchNorm momentum\n",
    "    momentum = initial_momentum * np.exp(\n",
    "        -epoch/num_epochs * np.log(initial_momentum/final_momentum)\n",
    "    )\n",
    "    model_run_train.set_bn_momentum(momentum)\n",
    "\n",
    "    # Save training curves after every epoch, as .png images\n",
    "    if epoch >= num_epochs:\n",
    "        plt.figure()\n",
    "        epoch_x = np.arange(3, len(losses_3d_train)) + 1\n",
    "        plt.plot(epoch_x, losses_3d_train[3:], '--', color='C0')\n",
    "        plt.plot(epoch_x, losses_3d_train_eval[3:], color='C0')\n",
    "        plt.plot(epoch_x, losses_3d_valid[3:], color='C1')\n",
    "        plt.legend(['3d train', '3d train (eval)', '3d valid (eval)'])\n",
    "        plt.ylabel('MPJPE (mm)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.xlim((3, epoch))\n",
    "        plt.savefig('loss_plots/' + str(epoch) + '_loss_3d.png')\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runningpose_epoch_51.bin\n",
      "Saving checkpoint to runningpose_epoch_51.bin\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "chk_path = os.path.join('runningpose_epoch_{}.bin'.format(epoch))\n",
    "print(chk_path)\n",
    "print('Saving checkpoint to', chk_path)\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'lr': lr,\n",
    "    'random_state': train_generator.random_state(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_run': model_run_train.state_dict(),\n",
    "}, chk_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vizualize the predictions on one of our validation videos\n",
    "> This is highly temporary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# from runningpose.core.skeleton import Skeleton\n",
    "# runningpose_skeleton = Skeleton(\n",
    "#     parents=[1, 2, 5, 1, 1, -1, 5, 16, 5, 17, 3, 4, 10, 11, 6, 8, 14, 15],\n",
    "#     joints_left=[3, 6, 7, 10, 12, 14, 16], \n",
    "#     joints_right=[4, 8, 9, 11, 13, 15, 17]\n",
    "# )\n",
    "# h36m_skeleton = Skeleton(\n",
    "#        parents=[-1, 0, 1, 2, 3, 4, 0, 6, 7],\n",
    "#        joints_left=[6, 7, 8, 9, 10],\n",
    "#        joints_right=[1, 2, 3, 4, 5]\n",
    "# )\n",
    "# # Load checkpoint\n",
    "# print('Loading checkpoint')\n",
    "# checkpoint = torch.load('runningpose_earlystop.bin', \n",
    "#                         map_location=lambda storage,\n",
    "#                         loc: storage)\n",
    "# print('This model was trained for {} epochs'.format(checkpoint['epoch']))\n",
    "\n",
    "# model_run = TemporalModel(\n",
    "#     num_joints_in, in_features, num_joints_out, filter_widths, causal, \n",
    "#     dropout, channels\n",
    "# )\n",
    "# if torch.cuda.is_available():\n",
    "#     model_run = model_run.cuda()\n",
    "\n",
    "# model_run.load_state_dict(checkpoint['model_run'])\n",
    "\n",
    "# from runningpose.core.runningpose_dataset import runningpose_cameras_extrinsic_params\n",
    "# from runningpose.core.camera import camera_to_world_miqus, image_coordinates\n",
    "# from runningpose.core.visualization import render_animation\n",
    "# tindra_cam1 = keypoints_2D_val['miqus1_Tindra_01.avi']['custom']\n",
    "# josef_cam1 = keypoints_2D['miqus1_Josef_01.avi']['custom']\n",
    "# gen = UnchunkedGenerator(\n",
    "#     cameras=None, poses_3d=None, poses_2d=josef_cam1,\n",
    "#     pad=pad, augment=False,\n",
    "#     kps_left=kps_left, kps_right=kps_right, \n",
    "#     joints_left=joints_left, joints_right=joints_right\n",
    "# )\n",
    "# predicted = 0\n",
    "# data_world = 0\n",
    "# print('Rendering...')\n",
    "# with torch.no_grad():\n",
    "#     # Load the newly trained network\n",
    "#     model_run.eval()\n",
    "\n",
    "#     for _, _, batch2d in gen.next_epoch():\n",
    "#         inputs_2d_valid = torch.from_numpy(batch2d.astype('float32'))\n",
    "#         if torch.cuda.is_available():\n",
    "#             inputs_2d_valid = inputs_2d_valid.cuda()\n",
    "\n",
    "#         # Predict 3D poses (forward)\n",
    "#         predicted_3d_pos = model_run(inputs_2d_valid)\n",
    "#         # Convert predicted 3d poses to world coordinates. \n",
    "#         predicted_3d_pos = predicted_3d_pos.cpu().detach().numpy()[0]\n",
    "#         predicted_3d_pos = predicted_3d_pos.transpose(1, 0, 2)\n",
    "#         predicted = predicted_3d_pos\n",
    "    \n",
    "#         # Get camera parameters.\n",
    "#         R = runningpose_cameras_extrinsic_params[0]['rotation']\n",
    "#         T = np.array([runningpose_cameras_extrinsic_params[0]['translation']]).T\n",
    "#         data_3D_world = []\n",
    "#         for keypoint in predicted_3d_pos:\n",
    "#             data_3D_world.append(camera_to_world_miqus(keypoint, R, T))\n",
    "\n",
    "#         data_3D_world = np.array(data_3D_world).transpose(1, 0, 2)\n",
    "#         data_3D_world -= np.min(data_3D_world[:, :, 2])\n",
    "        \n",
    "#         anim_output = {'Reconstruction': data_3D_world}\n",
    "#         input_keypoints = image_coordinates(josef_cam1[0][..., :2], w=1920, h=1088)\n",
    "        \n",
    "#         render_animation(\n",
    "#             input_keypoints, keypoints_2D_metadata, anim_output,\n",
    "#             h36m_skeleton, 85, 3000, 70, 'josef01_cam1.mp4', size=12, \n",
    "#             input_video_path='Josef_01_Miqus_14.avi', viewport=(1920, 1088)\n",
    "#         )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 439.29826206, -128.26596154,  385.53120254],\n",
       "       [ 362.83286278,  115.7493027 ,  329.98524554],\n",
       "       [ 105.64155381,   94.50332733,  118.17327544],\n",
       "       [ 464.2673625 ,   48.45052284,  213.03966273],\n",
       "       [ 265.43539413,   38.97675975,  437.7394667 ],\n",
       "       [   0.        ,    0.        ,    0.        ],\n",
       "       [ 185.04722421,  -66.97927267,  -49.25657858],\n",
       "       [ 133.75705487, -461.62530543, -145.99700319],\n",
       "       [ -90.03607797, -160.77142961,  120.01974104],\n",
       "       [-245.45213035, -451.36628365, -192.0049661 ],\n",
       "       [ 471.48228726, -221.50823594,   47.16161046],\n",
       "       [ 153.45110403, -250.22342412,  391.5758996 ],\n",
       "       [ 562.81530739, -435.21547956,  -11.55515732],\n",
       "       [ 126.54936129, -490.87431228,  451.34356865],\n",
       "       [ 353.81436485, -303.96792821,  122.87772211],\n",
       "       [  32.84878318, -494.134782  ,   83.64051776],\n",
       "       [ 112.08047204, -340.34488634, -131.66719718],\n",
       "       [-194.24631391, -348.23674884, -143.61948339]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = keypoints_3D['Josef1_Camera1_170Hz_3D_keypoints']\n",
    "input[0, :, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
