{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.format_qtmdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format qtmdata\n",
    ">  Format the Qualisys sports data preprocessed from Matlab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Reformat qtmdata so that we can train.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--data-file', \n",
    "        dest='data_file',\n",
    "        help='qtm text file that has been formated in matlab', \n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output-dir',\n",
    "        dest='output_dir',\n",
    "        help='directory for reformated keypoint data (default: ./)',\n",
    "        default='./',\n",
    "        type=str\n",
    "    )\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    Loads the qtm data then removes unwanted keypoints.\n",
    "    Then it infers new keypoints adds them. \n",
    "    Further we scale the dataset using a the norm vector between the \n",
    "    root and 'SpineThoracic2'. \n",
    "    \"\"\"\n",
    "    # Loads the textfiles\n",
    "    labels_np = np.loadtxt('qtm_labels_py.txt', dtype = 'str')\n",
    "    data_3D = np.loadtxt(args.data_file, dtype = 'float', delimiter= ',')\n",
    "\n",
    "    # Reformats the data to a dataframe\n",
    "    data_3D = pd.DataFrame(data_3D, index=labels_np).T\n",
    "\n",
    "    # Remove unwanted keypoints\n",
    "    data_3D = data_3D.drop(\n",
    "        columns=[\n",
    "            'HeadL', 'HeadR', 'Chest', 'LThighFrontLow', 'RThighFrontLow', \n",
    "            'LShinFrontHigh', 'RShinFrontHigh', 'LForefoot5', 'RForefoot5', \n",
    "            'LHeelBack', 'RHeelBack', 'LArm', 'RArm'\n",
    "        ]\n",
    "    )\n",
    "    # Create \"new\" keypoints by finding the mean between some specific keypoints\n",
    "    left_elbow_3D = data_3D.loc[:, ['LElbowOut','LElbowIn']].mean(axis=1)\n",
    "    right_elbow_3D = data_3D.loc[:, ['RElbowOut','RElbowIn']].mean(axis=1)\n",
    "\n",
    "    left_wrist_3D = data_3D.loc[:, ['LWristIn','LWristOut']].mean(axis=1)\n",
    "    right_wrist_3D = data_3D.loc[:, ['RWristOut','RWristIn']].mean(axis=1)\n",
    "\n",
    "    left_knee_3D = data_3D.loc[:, ['LKneeOut','LKneeIn']].mean(axis=1)\n",
    "    right_knee_3D = data_3D.loc[:, ['RKneeOut','RKneeIn']].mean(axis=1)\n",
    "\n",
    "    left_waist_3D = data_3D.loc[:, ['WaistLFront','WaistL']].mean(axis=1)\n",
    "    right_waist_3D = data_3D.loc[:, ['WaistRFront','WaistR']].mean(axis=1)\n",
    "\n",
    "    # Remove the keypoints that was taken as a mean\n",
    "    data_3D = data_3D.drop(\n",
    "        columns=[\n",
    "            'LElbowOut','LElbowIn', 'RElbowOut','RElbowIn', 'LWristIn', \n",
    "            'LWristOut', 'RWristIn','RWristOut', 'LKneeIn', 'LKneeOut', \n",
    "            'RKneeIn', 'RKneeOut', 'WaistLFront', 'WaistL', 'WaistRFront', \n",
    "            'WaistR'\n",
    "        ]\n",
    "    )\n",
    "    # Adds the new keypoint data to the dataframe\n",
    "    data_3D['LElbow'] = left_elbow_3D\n",
    "    data_3D['RElbow'] = right_elbow_3D\n",
    "    data_3D['LWrist'] = left_wrist_3D\n",
    "    data_3D['RWrist'] = right_wrist_3D\n",
    "    data_3D['LKnee'] = left_knee_3D\n",
    "    data_3D['RKnee'] = right_knee_3D\n",
    "    data_3D['LWaist'] = left_waist_3D\n",
    "    data_3D['RWaist'] = right_waist_3D\n",
    "\n",
    "    # Convert all the data relative to the root 'WaistBack'\n",
    "    data_3D = data_3D.subtract(data_3D['WaistBack'], axis=0)\n",
    "    \n",
    "    # Scale each frame with a norm vector\n",
    "    data_3D_scaled = [] \n",
    "    for i in range(0, data_3D.shape[0], 3):\n",
    "        # Calculates the scale factor for each frame\n",
    "        norm_vector = np.sqrt(np.square(\n",
    "            data_3D['SpineThoracic2'].iloc[i:i+3]).sum(axis=0)\n",
    "        )\n",
    "        data_3D_scaled.append(data_3D.iloc[i:i+3].divide(norm_vector))\n",
    "\n",
    "    # Reformat to dataframe again after all data has been scaled\n",
    "    data_3D = pd.concat(data_3D_scaled, ignore_index=True)\n",
    "\n",
    "    # Remove the y-dimension to get the 2D data for side cam. \n",
    "    # OBS! This may vary between camera angels.\n",
    "    data_2D = data_3D.drop(index=range(1, data_3D.shape[0], 3))\n",
    "    \n",
    "    # Creates output names that depends on the name of the data file \n",
    "    data_file_name =os.path.basename(os.path.normpath(args.data_file)).rsplit(\".\")[0]\n",
    "    out_2D = os.path.join(args.output_dir, data_file_name + '_2D_keypoints.csv')\n",
    "    out_3D = os.path.join(args.output_dir, data_file_name + '_3D_keypoints.csv')\n",
    "    \n",
    "    # Save the keypoint data as csv files\n",
    "    pd.DataFrame.to_csv(data_2D, path_or_buf=out_2D)\n",
    "    pd.DataFrame.to_csv(data_3D, path_or_buf=out_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "try: from nbdev.imports import IN_NOTEBOOK\n",
    "except: IN_NOTEBOOK=False\n",
    "\n",
    "if __name__ == '__main__' and not IN_NOTEBOOK:\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_model.ipynb.\n",
      "Converted 01_loss.ipynb.\n",
      "Converted 02_skeleton.ipynb.\n",
      "Converted 03_mocap_dataset.ipynb.\n",
      "Converted 04_h36m_dataset.ipynb.\n",
      "Converted 05_camera.ipynb.\n",
      "Converted 06_quaternion.ipynb.\n",
      "Converted 07_utils.ipynb.\n",
      "Converted 08_generators.ipynb.\n",
      "Converted 09_custom_dataset.ipynb.\n",
      "Converted 10_visualization.ipynb.\n",
      "Converted 11_arguments.ipynb.\n",
      "Converted 12_data_utils.ipynb.\n",
      "Converted 13_prepare_data_2d_custom.ipynb.\n",
      "Converted 14_infer_video.ipynb.\n",
      "Converted 15_prepare_data_COCO.ipynb.\n",
      "Converted 16_pycococreatortools.ipynb.\n",
      "Converted 17_format_qtmdata.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
