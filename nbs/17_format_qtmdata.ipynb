{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.format_qtmdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format qtmdata\n",
    ">  Format the Qualisys sports data preprocessed from Matlab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Reformat qtmdata so that we can train.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--data-file', \n",
    "        dest='data_file',\n",
    "        help='qtm text file that has been formated in matlab', \n",
    "        type=str\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output-dir',\n",
    "        dest='output_dir',\n",
    "        help='directory for reformated keypoint data (default: ./)',\n",
    "        default='./',\n",
    "        type=str\n",
    "    )\n",
    "    \n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def main(args):\n",
    "    \"\"\"\n",
    "    Loads the qtm data then removes unwanted keypoints.\n",
    "    Then it infers new keypoints adds them. \n",
    "    Further we scale the dataset using a the norm vector between the \n",
    "    root and 'SpineThoracic2'. \n",
    "    \"\"\"\n",
    "    # Loads the textfiles\n",
    "    labels_np = np.loadtxt('qtm_labels_py.txt', dtype = 'str')\n",
    "    data_3D = np.loadtxt(args.data_file, dtype = 'float', delimiter= ',')\n",
    "\n",
    "    # Reformats the data to a dataframe\n",
    "    data_3D = pd.DataFrame(data_3D, index=labels_np).T\n",
    "\n",
    "    # Remove unwanted keypoints\n",
    "    data_3D = data_3D.drop(\n",
    "        columns=[\n",
    "            'HeadL', 'HeadR', 'Chest', 'LThighFrontLow', 'RThighFrontLow', \n",
    "            'LShinFrontHigh', 'RShinFrontHigh', 'LForefoot5', 'RForefoot5', \n",
    "            'LHeelBack', 'RHeelBack', 'LArm', 'RArm','WaistLFront', 'WaistL', \n",
    "            'WaistRFront', 'WaistR'\n",
    "        ]\n",
    "    )\n",
    "    # Create \"new\" keypoints by finding the mean between some specific keypoints\n",
    "    left_elbow_3D = data_3D.loc[:, ['LElbowOut','LElbowIn']].mean(axis=1)\n",
    "    right_elbow_3D = data_3D.loc[:, ['RElbowOut','RElbowIn']].mean(axis=1)\n",
    "\n",
    "    left_wrist_3D = data_3D.loc[:, ['LWristIn','LWristOut']].mean(axis=1)\n",
    "    right_wrist_3D = data_3D.loc[:, ['RWristOut','RWristIn']].mean(axis=1)\n",
    "\n",
    "    left_knee_3D = data_3D.loc[:, ['LKneeOut','LKneeIn']].mean(axis=1)\n",
    "    right_knee_3D = data_3D.loc[:, ['RKneeOut','RKneeIn']].mean(axis=1)\n",
    "\n",
    "    #left_waist_3D = data_3D.loc[:, ['WaistLFront','WaistL']].mean(axis=1)\n",
    "    #right_waist_3D = data_3D.loc[:, ['WaistRFront','WaistR']].mean(axis=1)\n",
    "\n",
    "    left_ankle_3D = data_3D.loc[:, ['LAnkleOut','LAnkleIn']].mean(axis=1)\n",
    "    right_ankle_3D = data_3D.loc[:, ['RAnkleOut','RAnkleIn']].mean(axis=1)\n",
    "\n",
    "    # Remove the keypoints that was taken as a mean\n",
    "    data_3D = data_3D.drop(\n",
    "        columns=[\n",
    "            'LElbowOut','LElbowIn', 'RElbowOut','RElbowIn', \n",
    "            'LWristIn','LWristOut', 'RWristIn','RWristOut', \n",
    "            'LKneeIn', 'LKneeOut','RKneeIn', 'RKneeOut',\n",
    "            'LAnkleOut','LAnkleIn','RAnkleOut','RAnkleIn'\n",
    "        ]\n",
    "    )\n",
    "    # Adds the new keypoint data to the dataframe\n",
    "    data_3D['LElbow'] = left_elbow_3D\n",
    "    data_3D['RElbow'] = right_elbow_3D\n",
    "    data_3D['LWrist'] = left_wrist_3D\n",
    "    data_3D['RWrist'] = right_wrist_3D\n",
    "    data_3D['LKnee'] = left_knee_3D\n",
    "    data_3D['RKnee'] = right_knee_3D\n",
    "    #data_3D['LWaist'] = left_waist_3D\n",
    "    #data_3D['RWaist'] = right_waist_3D\n",
    "    data_3D['LAnkle'] = left_ankle_3D\n",
    "    data_3D['RAnkle'] = right_ankle_3D\n",
    "\n",
    "    # Convert all the data relative to the root 'WaistBack'\n",
    "    #data_3D = data_3D.subtract(data_3D['WaistBack'], axis=0)\n",
    "    \n",
    "    # Scale each frame with a norm vector\n",
    "    #data_3D_scaled = [] \n",
    "    #for i in range(0, data_3D.shape[0], 3):\n",
    "        # Calculates the scale factor for each frame\n",
    "        #norm_vector = np.sqrt(np.square(\n",
    "            #data_3D['SpineThoracic2'].iloc[i:i+3]).sum(axis=0)\n",
    "        #)\n",
    "        #data_3D_scaled.append(data_3D.iloc[i:i+3].divide(norm_vector))\n",
    "\n",
    "    # Reformat to dataframe again after all data has been scaled\n",
    "    #data_3D = pd.concat(data_3D_scaled, ignore_index=True)\n",
    "\n",
    "    # Remove the y-dimension to get the 2D data for side cam. \n",
    "    # OBS! This may vary between camera angels.\n",
    "    # TODO: Add argument argument for which dim to drop to 2D\n",
    "    data_2D = data_3D.drop(index=range(1, data_3D.shape[0], 3))\n",
    "    \n",
    "    # Creates output names that depends on the name of the data file \n",
    "    data_file_name = os.path.basename(os.path.normpath(args.data_file)).rsplit(\".\")[0]\n",
    "    out_2D = os.path.join(args.output_dir, data_file_name + '_2D_keypoints.csv')\n",
    "    out_3D = os.path.join(args.output_dir, data_file_name + '_3D_keypoints.csv')\n",
    "    \n",
    "    # Save the keypoint data as csv files\n",
    "    pd.DataFrame.to_csv(data_2D, path_or_buf=out_2D)\n",
    "    pd.DataFrame.to_csv(data_3D, path_or_buf=out_3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--data-file DATA_FILE]\n",
      "                             [--output-dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9011 --control=9009 --hb=9008 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"f8abbca2-3eec-47de-950e-f847a25a676a\" --shell=9010 --transport=\"tcp\" --iopub=9012 --f=C:\\Users\\Familjen\\AppData\\Local\\Temp\\tmp-9164POGX1yT326mU.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Familjen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "try: from nbdev.imports import IN_NOTEBOOK\n",
    "except: IN_NOTEBOOK=False\n",
    "\n",
    "if __name__ == '__main__' and not IN_NOTEBOOK:\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_model.ipynb.\n",
      "Converted 01_loss.ipynb.\n",
      "Converted 02_skeleton.ipynb.\n",
      "Converted 03_mocap_dataset.ipynb.\n",
      "Converted 04_h36m_dataset.ipynb.\n",
      "Converted 05_camera.ipynb.\n",
      "Converted 06_quaternion.ipynb.\n",
      "Converted 07_utils.ipynb.\n",
      "Converted 08_generators.ipynb.\n",
      "Converted 09_custom_dataset.ipynb.\n",
      "Converted 10_visualization.ipynb.\n",
      "Converted 11_arguments.ipynb.\n",
      "Converted 12_data_utils.ipynb.\n",
      "Converted 13_prepare_data_2d_custom.ipynb.\n",
      "Converted 14_infer_video.ipynb.\n",
      "Converted 15_prepare_data_COCO.ipynb.\n",
      "Converted 16_pycococreatortools.ipynb.\n",
      "Converted 17_format_qtmdata.ipynb.\n",
      "No export destination, ignored:\n",
      "#export\n",
      "# Some basic setup:\n",
      "# Setup detectron2 logger\n",
      "import detectron2\n",
      "from detectron2.utils.logger import setup_logger\n",
      "setup_logger()\n",
      "\n",
      "# Import some common libraries\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import os, json, cv2, random\n",
      "\n",
      "# Import some common detectron2 utilities\n",
      "from detectron2 import model_zoo\n",
      "from detectron2.engine import DefaultTrainer\n",
      "from detectron2.config import get_cfg\n",
      "from detectron2.utils.visualizer import Visualizer\n",
      "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
      "from detectron2.data.datasets import register_coco_instances\n",
      "No export destination, ignored:\n",
      "#export\n",
      "# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n",
      "# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n",
      "No export destination, ignored:\n",
      "#export\n",
      "# TODO: Add names from dataframe. \n",
      "keypoint_names = [\n",
      "    'RWaist', # 1\n",
      "    'LWaist', # 2\n",
      "    'RKnee',  # 3\n",
      "    'LKnee',  # 4\n",
      "    'RWrist', # 5\n",
      "    'LWrist', # 6\n",
      "    'RElbow', # 7\n",
      "    'LElbow', # 8\n",
      "    'RForefoot2', # 9\n",
      "    'RAnkleOut', # 10\n",
      "    'LForefoot2', # 11\n",
      "    'LAnkleOut', # 12 \n",
      "    'WaistBack', # 13\n",
      "    'RHand2', # 14\n",
      "    'RShoulderTop', # 15\n",
      "    'LHand2', # 16\n",
      "    'LShoulderTop', # 17\n",
      "    'SpineThoracic12', # 18\n",
      "    'SpineThoracic2', # 19\n",
      "    'HeadFront' # 20\n",
      "]\n",
      "keypoint_flip_map = [\n",
      "    ('RShoulderTop', 'LShoulderTop'), ('RElbow', 'LElbow'), ('RWrist', 'LWrist'),\n",
      "    ('RHand2', 'LHand2'), ('RWaist', 'LWaist'), ('RKnee', 'LKnee'), \n",
      "    ('RAnkleOut', 'LAnkleOut'), ('RForefoot2', 'LForefoot2')\n",
      "]\n",
      "# MetadataCatalog.get(\"my_dataset_train\").keypoint_names = keypoint_names\n",
      "# MetadataCatalog.get(\"my_dataset_train\").keypoint_flip_map = keypoint_flip_map\n",
      "# MetadataCatalog.get(\"my_dataset_train\").evaluator_type = \"coco\"\n",
      "\n",
      "# dataset_dicts = DatasetCatalog.get(\"my_dataset_train\")\n",
      "No export destination, ignored:\n",
      "#export \n",
      "cfg = get_cfg()\n",
      "cfg.merge_from_file(model_zoo.get_config_file(\n",
      "    \"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\"))\n",
      "# cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
      "# cfg.DATASETS.TEST = (\"my_dataset_test\")\n",
      "cfg.DATALOADER.NUM_WORKERS = 2\n",
      "# Let training initialize from model zoo\n",
      "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
      "    \"COCO-Keypoints/keypoint_rcnn_R_101_FPN_3x.yaml\") \n",
      "cfg.SOLVER.IMS_PER_BATCH = 2\n",
      "cfg.SOLVER.BASE_LR = 0.00025  # Pick a good LR\n",
      "cfg.SOLVER.MAX_ITER = 300    # 300 iterations\n",
      "cfg.SOLVER.STEPS = []        # Do not decay learning rate\n",
      "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 # NOTE: Increase if possible  \n",
      "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Only have one class\n",
      "cfg.MODEL.RETINANET.NUM_CLASSES = 1\n",
      "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 20\n",
      "# More info on below see section 1.3: https://cocodataset.org/#keypoints-eval\n",
      "cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((20, 1), dtype=float).tolist()\n",
      "\n",
      "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
      "No export destination, ignored:\n",
      "#export\n",
      "trainer = DefaultTrainer(cfg) \n",
      "trainer.resume_or_load(resume=False)\n",
      "trainer.train()\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n",
      "Warning: Exporting to \"None.py\" but this module is not part of this build\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13783/242600319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnbdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mnotebook2script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/media/richard/SecondLinux/minconda3/envs/fastai/lib/python3.7/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36mnotebook2script\u001b[0;34m(fname, silent, to_dict, bare)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_mod_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_notebook2script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbare\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0madd_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lib_path\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/richard/SecondLinux/minconda3/envs/fastai/lib/python3.7/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36m_notebook2script\u001b[0;34m(fname, modules, silent, to_dict, bare)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_from_future_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mto_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_add2all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"'{f}'\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' +$'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/richard/SecondLinux/minconda3/envs/fastai/lib/python3.7/site-packages/nbdev/export.py\u001b[0m in \u001b[0;36m_add2all\u001b[0;34m(fname, names, line_width)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0mtw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequent_indent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_long_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0mre_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_re__all__def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mre_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mtext_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{text[start:end-1]}{'' if text[end-2]=='[' else ', '}{', '.join(names)}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_all\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
