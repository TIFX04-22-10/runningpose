{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core.custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom dataset\n",
    "> Implement a custom dataset for training and testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from runningpose.core.camera import (image_coordinates,\n",
    "                                     normalize_screen_coordinates)\n",
    "from runningpose.core.h36m_dataset import h36m_skeleton\n",
    "from runningpose.core.mocap_dataset import MocapDataset\n",
    "from runningpose.core.skeleton import Skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "custom_camera_params = {\n",
    "    'id': None,\n",
    "    'res_w': None, # Pulled from metadata\n",
    "    'res_h': None, # Pulled from metadata\n",
    "    \n",
    "    # Dummy camera parameters (taken from Human3.6M), only for visualization purposes\n",
    "    'azimuth': 70, # Only used for visualization\n",
    "    'orientation': [0.1407056450843811, -0.1500701755285263, -0.755240797996521, 0.6223280429840088],\n",
    "    'translation': [1841.1070556640625, 4955.28466796875, 1563.4454345703125],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class CustomDataset(MocapDataset):\n",
    "    \"\"\"Creates a custom dataset with the Human36m skeleton.\"\"\"\n",
    "    def __init__(self, detections_path, remove_static_joints=True):\n",
    "        super().__init__(fps=None, skeleton=h36m_skeleton)        \n",
    "        self._cameras = {}\n",
    "        self._data = {}\n",
    "\n",
    "        # Load serialized dataset\n",
    "        data = np.load(detections_path, allow_pickle=True)\n",
    "        resolutions = data['metadata'].item()['video_metadata']\n",
    "\n",
    "        for video_name, res in resolutions.items():\n",
    "            cam = {}\n",
    "            cam.update(custom_camera_params)\n",
    "            cam['orientation'] = np.array(cam['orientation'], dtype='float32')\n",
    "            cam['translation'] = np.array(cam['translation'], dtype='float32')\n",
    "            cam['translation'] = cam['translation']/1000 # mm to meters\n",
    "            \n",
    "            cam['id'] = video_name\n",
    "            cam['res_w'] = res['w']\n",
    "            cam['res_h'] = res['h']\n",
    "            \n",
    "            self._cameras[video_name] = [cam]\n",
    "        \n",
    "            self._data[video_name] = {'custom': { 'cameras': cam } }\n",
    "                \n",
    "        if remove_static_joints:\n",
    "            # Bring the skeleton to 17 joints instead of the original 32\n",
    "            self.remove_joints([\n",
    "                4, 5, 9, 10, 11, 16, 20, 21, 22, 23, 24, 28, 29, 30, 31\n",
    "            ])\n",
    "            \n",
    "            # Rewire shoulders to the correct parents\n",
    "            self._skeleton._parents[11] = 8\n",
    "            self._skeleton._parents[14] = 8\n",
    "            \n",
    "    def supports_semi_supervised(self):\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_model.ipynb.\n",
      "Converted 01_loss.ipynb.\n",
      "Converted 02_skeleton.ipynb.\n",
      "Converted 03_mocap_dataset.ipynb.\n",
      "Converted 04_h36m_dataset.ipynb.\n",
      "Converted 05_camera.ipynb.\n",
      "Converted 06_quaternion.ipynb.\n",
      "Converted 07_utils.ipynb.\n",
      "Converted 08_generators.ipynb.\n",
      "Converted 09_custom_dataset.ipynb.\n",
      "Converted 10_visualization.ipynb.\n",
      "Converted 11_arguments.ipynb.\n",
      "Converted 12_data_utils.ipynb.\n",
      "Converted 13_prepare_data_2d_custom.ipynb.\n",
      "Converted 14_infer_video.ipynb.\n",
      "Converted 15_prepare_data_COCO.ipynb.\n",
      "Converted 16_pycococreatortools.ipynb.\n",
      "Converted 17_format_qtmdata.ipynb.\n",
      "Converted 18_runningpose_dataset.ipynb.\n",
      "Converted 19_train_detectron2.ipynb.\n",
      "Converted 20_transfer_model.ipynb.\n",
      "Converted 21_prepare_data_3d.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('fastai')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
