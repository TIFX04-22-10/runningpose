{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.prepare_data_COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data to COCO\n",
    "> Reformats the Qualisys keypointdata to COCO-keypoint format i.e add object detection (segementation) and return in json format. Run this file while in the data directory.\n",
    "\n",
    "To test enter: ``` python prepare_data_COCO.py \\ --image-ext mp4 \\ richardrun ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# Workaround for the relative import error, \n",
    "# removes the dot (.) infront when converting to .py script.\n",
    "try:\n",
    "    from runningpose.data.inference import infer_video\n",
    "    from runningpose.data.pycoco import pycococreatortools as pycoco\n",
    "except:\n",
    "    from inference import infer_video\n",
    "    from pycoco import pycococreatortools as pycoco\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# The INFO section contains high level information about the dataset.\n",
    "INFO = {\n",
    "    \"description\": \"Runningpose Dataset\",\n",
    "    \"url\": \"https://github.com/TIFX04-22-10/runningpose\",\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"year\": 2022,\n",
    "    \"contributor\": \"svenssona\",\n",
    "    \"date_created\": datetime.datetime.utcnow().isoformat(' ')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# The “licenses” section contains a list of image licenses \n",
    "# that apply to images in the dataset.\n",
    "LICENSES = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Creative Commons Attribution 4.0 International\",\n",
    "        \"url\": \"https://dataworldsupport.atlassian.net/servicedesk/customer/portals\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# In the case of a person, “keypoints” indicate different body parts. \n",
    "# The “skeleton” indicates connections between points. \n",
    "# For example, [20, 19] means \"HeadFront\" connects to \"SpineThoracic2\".\n",
    "CATEGORIES = [\n",
    "   {\n",
    "        \"supercategory\": \"person\",\n",
    "        \"id\": 1,\n",
    "        \"name\": \"person\",\n",
    "        \"keypoints\": [\n",
    "            'RAnkle', # 1         #\n",
    "            'LAnkle', # 2         #\n",
    "            'RKnee',  # 3\n",
    "            'LKnee',  # 4\n",
    "            'RWrist', # 5\n",
    "            'LWrist', # 6\n",
    "            'RElbow', # 7\n",
    "            'LElbow', # 8\n",
    "            'RForefoot', # 9\n",
    "            'RTrochanterMajor', # 10       #\n",
    "            'LForefoot', # 11    \n",
    "            'LTrochanterMajor', # 12       #\n",
    "            'WaistBack', # 13\n",
    "            'RShoulderTop', # 14\n",
    "            'LShoulderTop', # 15\n",
    "            'SpineThoracic12', # 16\n",
    "            'SpineThoracic2', # 17\n",
    "            'HeadFront' # 18\n",
    "        ],\n",
    "        \"skeleton\": [ # 17 connections in total\n",
    "            [18, 17], [17, 15], [17, 14], [17, 16], [16, 13], [13, 10], [13, 12], \n",
    "            [10, 3], [3, 1], [1, 9], [12, 4], [4, 2], [2, 11], [14, 7], \n",
    "            [7, 5], [15, 8], [8, 6]\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Reformat to COCO-keypoint json format.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--video_ext',\n",
    "        dest='video_ext',\n",
    "        help='video file name extension (default: avi)',\n",
    "        default='avi',\n",
    "        type=str\n",
    "    )\n",
    "    #parser.add_argument(\n",
    "    #    '--video_or_folder', \n",
    "    #    help='video or folder of videos', \n",
    "    #    default=\"richardrun2\",\n",
    "    #    type=str\n",
    "    #)\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--train_valid_test',\n",
    "        help='Used as training, validation or test data (train, valid, test)', \n",
    "        default=0,\n",
    "        type=str\n",
    "    )\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def main(args):\n",
    "    print(os.getcwd())\n",
    "    \"\"\"\n",
    "    Runs inference on the video files and saves the dataset in json \n",
    "    file format. Predicts the boundary box and segementation points.\n",
    "    Then adds the Qualisys keypoint data. \n",
    "    \n",
    "    OBS! Make sure video_name matches csv file for 2D keypoints and \n",
    "    that they are in the same folder. \n",
    "    \"\"\"\n",
    "    # Create a detectron2 config and DefaultPredictor to run inference on video.\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\n",
    "        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7 # Set threshold for this model.\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\n",
    "        \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\") \n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    \n",
    "    # Load the video folder in which we should predict.\n",
    "    if args.train_valid_test=='train':\n",
    "        path = 'data_for_COCO/test_data'\n",
    "    elif args.train_valid_test=='valid':\n",
    "        path = 'data_for_COCO/validation_data'\n",
    "    elif args.train_valid_test=='test':\n",
    "        path = 'data_for_COCO/test_data'\n",
    "    else:\n",
    "        raise KeyError('Invalid data specification, please choose train, valid, or test')\n",
    "    \n",
    "    if os.path.isdir(args.video_or_folder):\n",
    "        video_list = glob.iglob(path + '/*.' + args.video_ext)\n",
    "    else:\n",
    "        raise KeyError('Invalid path')\n",
    "    \n",
    "    #if os.path.isdir(args.video_or_folder):\n",
    "    #    video_list = glob.iglob(args.video_or_folder + '/*.' + args.video_ext)\n",
    "    #else:\n",
    "    #    video_list = [args.video_or_folder]\n",
    "\n",
    "    # Initialize coco_outputs, annotation id and video id:\n",
    "    coco_output = {\n",
    "        \"info\": INFO,\n",
    "        \"licenses\": LICENSES,\n",
    "        \"categories\": CATEGORIES,\n",
    "        \"images\": [],\n",
    "        \"annotations\": []\n",
    "    }\n",
    "    annotation_id = 0\n",
    "    video_id = 0 # Increases by 1e4 if we have multiple videos.\n",
    "    for video_name in video_list:\n",
    "        print(\"Processing {}\".format(video_name))\n",
    "         \n",
    "        # Make sure video_name matches csv file for 2D keypoints \n",
    "        # and that they are in the same folder. \n",
    "        # TODO: Change so that it only works for mp4\n",
    "        keypoints = get_COCO_keypoints(video_name)\n",
    "\n",
    "        annotation_id = 1\n",
    "        for frame_i, im in enumerate(infer_video.read_video(video_name)):\n",
    "            t = time.time()\n",
    "            outputs = predictor(im)[\"instances\"].to('cpu')\n",
    "            print(\"Frame {} processed in {:.3f}s\".format(frame_i, time.time()-t)) \n",
    "            # Filter out the person class from the prediction; \n",
    "            # 0 is the index for persons.\n",
    "            outputs = outputs[outputs.pred_classes == 0]\n",
    "\n",
    "            # Checks if image is \"empty or not\" and makes sure there \n",
    "            # is only one person detected.\n",
    "            if outputs.has(\"pred_boxes\") and len(outputs) == 1:\n",
    "                # Converts the tensors to a numpy arrays and slice away \n",
    "                # the person class.\n",
    "                bbox_tensor = outputs.pred_boxes.tensor.numpy()[0, :]\n",
    "                pred_masks_tensor = outputs.pred_masks.numpy()[0, :, :]\n",
    "                \n",
    "                # Create the image section, it contains the complete list \n",
    "                # of images in your dataset.\n",
    "                image_id = video_id + frame_i\n",
    "                image_info = pycoco.create_image_info(image_id, video_name, im)\n",
    "                coco_output[\"images\"].append(image_info)\n",
    "\n",
    "                # Create the annotations in the single person coco-keypoint format. \n",
    "                annotation_info = pycoco.create_annotation_info(\n",
    "                    annotation_id, \n",
    "                    image_id, \n",
    "                    pred_masks_tensor, \n",
    "                    bbox_tensor, \n",
    "                    keypoints=keypoints[frame_i]\n",
    "                )\n",
    "\n",
    "                coco_output[\"annotations\"].append(annotation_info)\n",
    "                annotation_id += 1\n",
    "                \n",
    "        video_id += 1e4\n",
    "    \n",
    "    output_name = 'COCO_data_json_files/' + args.train_valid_test\n",
    "\n",
    "    with open(\"{}.json\".format(output_name), \"w\") as output_json_file:\n",
    "        json.dump(coco_output, output_json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def get_COCO_keypoints(video_name):\n",
    "    \"\"\"\n",
    "    Loads the formated qtm data for the video (.mp4) and reformats it to \n",
    "    COCO format i.e list: [x, y, v, x, y, v, ... x, y, v], where v is a visual \n",
    "    parameter (0, 1, 2); whether the keypoint is visual and measured.  \n",
    "        0 = not measured.\n",
    "        1 = measured but not visual.\n",
    "        2 = measured and visual.\n",
    "\n",
    "    Returns a list of list with keypoints for each frame.\n",
    "    \"\"\"\n",
    "    file_path = video_name.replace(args.video_ext, \"_2D_keypoints.csv\")\n",
    "    data_2D = pd.read_csv(file_path, index_col=0)\n",
    "\n",
    "    keypoints = []\n",
    "    for i in range(0, data_2D.shape[0], 2):\n",
    "        x_keypoints = list(data_2D.iloc[i, :])\n",
    "        y_keypoints = list(data_2D.iloc[i+1, :])\n",
    "        keypoints_one_frame = []\n",
    "        while(y_keypoints):\n",
    "            # Remember that pop() reverses the order of the keypoints.\n",
    "            keypoints_one_frame.append(x_keypoints.pop())\n",
    "            keypoints_one_frame.append(y_keypoints.pop())\n",
    "            # Assume v=2 for lack of better way to do it. \n",
    "            keypoints_one_frame.append(2)\n",
    "        \n",
    "        keypoints.append(keypoints_one_frame)\n",
    "\n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "try: from nbdev.imports import IN_NOTEBOOK\n",
    "except: IN_NOTEBOOK=False\n",
    "\n",
    "if __name__ == '__main__' and not IN_NOTEBOOK:\n",
    "    setup_logger()\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_model.ipynb.\n",
      "Converted 01_loss.ipynb.\n",
      "Converted 02_skeleton.ipynb.\n",
      "Converted 03_mocap_dataset.ipynb.\n",
      "Converted 04_h36m_dataset.ipynb.\n",
      "Converted 05_camera.ipynb.\n",
      "Converted 06_quaternion.ipynb.\n",
      "Converted 07_utils.ipynb.\n",
      "Converted 08_generators.ipynb.\n",
      "Converted 09_custom_dataset.ipynb.\n",
      "Converted 10_visualization.ipynb.\n",
      "Converted 11_arguments.ipynb.\n",
      "Converted 12_data_utils.ipynb.\n",
      "Converted 13_prepare_data_2d_custom.ipynb.\n",
      "Converted 14_infer_video.ipynb.\n",
      "Converted 15_prepare_data_COCO.ipynb.\n",
      "Converted 16_pycococreatortools.ipynb.\n",
      "Converted 17_format_qtmdata.ipynb.\n",
      "Converted 18_runningpose_dataset.ipynb.\n",
      "Converted 19_train_detectron2.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
